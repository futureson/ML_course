{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qiming Sun\n",
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109.81967768  73.68895452  96.58434842 ...  58.32779473  74.38901745\n",
      "  51.59669261]\n",
      "[[ 1.          1.94406149]\n",
      " [ 1.          0.62753668]\n",
      " [ 1.          2.01244346]\n",
      " ...\n",
      " [ 1.         -0.64968792]\n",
      " [ 1.          0.69312469]\n",
      " [ 1.         -1.14970831]]\n",
      "[ 1.94406149  0.62753668  2.01244346 ... -0.64968792  0.69312469\n",
      " -1.14970831]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(y)\n",
    "print(tx)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # y = y.reshape(len(y),1)\n",
    "    # if w.ndim == 1:\n",
    "    #     w = w.reshape(len(w),1)\n",
    "    loss = np.mean((y - tx.dot(w))**2)/2\n",
    "    return loss\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "w = np.array([1,2]).T\n",
    "compute_loss(y,tx,w)\n",
    "# print(tx.dot(w))\n",
    "# print(tx@w)\n",
    "# print(tx*w)\n",
    "# print(np.multiply(tx,w))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    W = np.array(np.meshgrid(grid_w0, grid_w1)).transpose(1,0,2)\n",
    "    y = y.reshape(len(y),1)\n",
    "    losses = np.mean((y - tx.dot(W).reshape(len(tx),-1))**2, axis = 0).reshape(len(grid_w0),-1)\n",
    "    # losses = compute_loss(y,tx,W).reshape(len(grid_w0),-1)\n",
    "    return losses/2\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678253, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.045 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import grid\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # y = y.reshape(len(y),-1)\n",
    "    grid = -np.mean((y - tx.dot(w)).reshape(len(y),-1)*tx,axis=0)\n",
    "\n",
    "    return grid\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        grid = compute_gradient(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        w = w - gamma*grid\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=292.7434883040181, w0=52.32939220021051, w1=10.3479712434989\n",
      "GD iter. 1/49: loss=240.0455442213324, w0=54.425845180399975, w1=10.661145362647916\n",
      "GD iter. 2/49: loss=197.36020951435694, w0=56.312652862570495, w1=10.943002069882029\n",
      "GD iter. 3/49: loss=162.78508840170676, w0=58.01077977652396, w1=11.196673106392728\n",
      "GD iter. 4/49: loss=134.77924030046012, w0=59.539093999082084, w1=11.424977039252358\n",
      "GD iter. 5/49: loss=112.09450333845028, w0=60.9145767993844, w1=11.630450578826025\n",
      "GD iter. 6/49: loss=93.71986639922227, w0=62.152511319656476, w1=11.815376764442327\n",
      "GD iter. 7/49: loss=78.83641047844763, w0=63.266652387901345, w1=11.981810331496998\n",
      "GD iter. 8/49: loss=66.7808111826202, w0=64.26937934932172, w1=12.131600541846202\n",
      "GD iter. 9/49: loss=57.015775752999986, w0=65.17183361460008, w1=12.266411731160487\n",
      "GD iter. 10/49: loss=49.106097055007545, w0=65.9840424533506, w1=12.38774180154334\n",
      "GD iter. 11/49: loss=42.69925730963363, w0=66.71503040822606, w1=12.496938864887913\n",
      "GD iter. 12/49: loss=37.509717115880825, w0=67.37291956761396, w1=12.595216221898024\n",
      "GD iter. 13/49: loss=33.30618955894109, w0=67.96501981106309, w1=12.683665843207127\n",
      "GD iter. 14/49: loss=29.90133223781986, w0=68.4979100301673, w1=12.763270502385318\n",
      "GD iter. 15/49: loss=27.143397807711644, w0=68.97751122736109, w1=12.83491469564569\n",
      "GD iter. 16/49: loss=24.909470919324022, w0=69.4091523048355, w1=12.899394469580026\n",
      "GD iter. 17/49: loss=23.099990139730046, w0=69.79762927456247, w1=12.957426266120926\n",
      "GD iter. 18/49: loss=21.63431070825893, w0=70.14725854731674, w1=13.009654883007737\n",
      "GD iter. 19/49: loss=20.447110368767326, w0=70.46192489279558, w1=13.056660638205868\n",
      "GD iter. 20/49: loss=19.485478093779133, w0=70.74512460372654, w1=13.098965817884185\n",
      "GD iter. 21/49: loss=18.70655595103869, w0=71.0000043435644, w1=13.13704047959467\n",
      "GD iter. 22/49: loss=18.075629015418933, w0=71.22939610941847, w1=13.171307675134107\n",
      "GD iter. 23/49: loss=17.56457819756693, w0=71.43584869868714, w1=13.202148151119601\n",
      "GD iter. 24/49: loss=17.150627035106815, w0=71.62165602902894, w1=13.229904579506545\n",
      "GD iter. 25/49: loss=16.81532659351411, w0=71.78888262633656, w1=13.254885365054795\n",
      "GD iter. 26/49: loss=16.54373323582403, w0=71.93938656391342, w1=13.27736807204822\n",
      "GD iter. 27/49: loss=16.323742616095057, w0=72.0748401077326, w1=13.297602508342303\n",
      "GD iter. 28/49: loss=16.145550214114575, w0=72.19674829716986, w1=13.315813501006977\n",
      "GD iter. 29/49: loss=16.001214368510386, w0=72.30646566766339, w1=13.332203394405184\n",
      "GD iter. 30/49: loss=15.884302333571005, w0=72.40521130110757, w1=13.34695429846357\n",
      "GD iter. 31/49: loss=15.7896035852701, w0=72.49408237120733, w1=13.360230112116119\n",
      "GD iter. 32/49: loss=15.71289759914637, w0=72.57406633429711, w1=13.372178344403412\n",
      "GD iter. 33/49: loss=15.650765750386144, w0=72.64605190107793, w1=13.382931753461976\n",
      "GD iter. 34/49: loss=15.600438952890359, w0=72.71083891118064, w1=13.392609821614684\n",
      "GD iter. 35/49: loss=15.559674246918783, w0=72.7691472202731, w1=13.40132008295212\n",
      "GD iter. 36/49: loss=15.526654835081802, w0=72.8216246984563, w1=13.409159318155814\n",
      "GD iter. 37/49: loss=15.499909111493846, w0=72.8688544288212, w1=13.416214629839137\n",
      "GD iter. 38/49: loss=15.478245075387598, w0=72.91136118614959, w1=13.422564410354129\n",
      "GD iter. 39/49: loss=15.460697206141544, w0=72.94961726774515, w1=13.428279212817621\n",
      "GD iter. 40/49: loss=15.446483432052235, w0=72.98404774118116, w1=13.433422535034763\n",
      "GD iter. 41/49: loss=15.434970275039895, w0=73.01503516727357, w1=13.438051525030192\n",
      "GD iter. 42/49: loss=15.4256446178599, w0=73.04292385075672, w1=13.442217616026078\n",
      "GD iter. 43/49: loss=15.418090835544106, w0=73.06802366589157, w1=13.445967097922376\n",
      "GD iter. 44/49: loss=15.411972271868313, w0=73.09061349951294, w1=13.449341631629043\n",
      "GD iter. 45/49: loss=15.407016235290916, w0=73.11094434977217, w1=13.452378711965043\n",
      "GD iter. 46/49: loss=15.403001845663226, w0=73.12924211500547, w1=13.455112084267443\n",
      "GD iter. 47/49: loss=15.3997501900648, w0=73.14571010371544, w1=13.457572119339604\n",
      "GD iter. 48/49: loss=15.397116349030076, w0=73.16053129355441, w1=13.459786150904549\n",
      "GD iter. 49/49: loss=15.39498293779195, w0=73.1738703644095, w1=13.461778779312999\n",
      "GD: execution time=0.100 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([50, 10])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fd5f7ac19c4549a1a9322d4ea64841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    grid = -np.mean((y - tx.dot(w)).reshape(len(y),-1)*tx,axis=0)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        index = np.random.randint(0,len(y),batch_size)# select randomly batch_size element from y to compute loss and grid\n",
    "        ys,txs = y[index],tx[index] #batch from y\n",
    "        loss = compute_loss(ys,txs,w)\n",
    "        grid = compute_stoch_gradient(ys,txs,w)\n",
    "\n",
    "        w = w - gamma*grid\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=223.5743533174922, w0=52.083980500008664, w1=9.650016122432937\n",
      "SGD iter. 1/49: loss=303.622048942763, w0=54.48964811435199, w1=10.431132600207924\n",
      "SGD iter. 2/49: loss=213.56737347054724, w0=56.43905923978726, w1=10.246872797106331\n",
      "SGD iter. 3/49: loss=155.07427162720347, w0=58.070377041359244, w1=9.640693592296621\n",
      "SGD iter. 4/49: loss=99.04187652752407, w0=59.26783484752961, w1=9.93031573876055\n",
      "SGD iter. 5/49: loss=133.891306164201, w0=60.77285779179484, w1=10.915459161120049\n",
      "SGD iter. 6/49: loss=76.93243639227562, w0=61.83877991379091, w1=11.315037563341617\n",
      "SGD iter. 7/49: loss=105.82740973404722, w0=63.215986817831165, w1=11.353382632507728\n",
      "SGD iter. 8/49: loss=107.2717077835191, w0=64.5430594154338, w1=11.81052642604242\n",
      "SGD iter. 9/49: loss=79.20818375567623, w0=65.46842676483676, w1=12.179143296178765\n",
      "SGD iter. 10/49: loss=48.07324512160052, w0=66.10764353268577, w1=12.412352453871046\n",
      "SGD iter. 11/49: loss=29.550816156808388, w0=66.76814776260382, w1=12.633016853994826\n",
      "SGD iter. 12/49: loss=26.64181823498439, w0=67.35790808553205, w1=12.711411556057515\n",
      "SGD iter. 13/49: loss=21.30313741688057, w0=67.80923026192492, w1=13.002899074435419\n",
      "SGD iter. 14/49: loss=45.456181896959265, w0=68.5542494127298, w1=13.382900615656872\n",
      "SGD iter. 15/49: loss=13.70052445454707, w0=68.95036400620711, w1=13.543896969732655\n",
      "SGD iter. 16/49: loss=14.028511049303711, w0=69.30975562380661, w1=13.55564583438488\n",
      "SGD iter. 17/49: loss=27.873902079152696, w0=69.89207093694819, w1=13.370488192664846\n",
      "SGD iter. 18/49: loss=14.854640939476184, w0=70.18638872839115, w1=13.677318131010876\n",
      "SGD iter. 19/49: loss=20.333464490473233, w0=70.46223954722652, w1=13.50952869369866\n",
      "SGD iter. 20/49: loss=9.985598457930028, w0=70.61048971708198, w1=13.55069063871226\n",
      "SGD iter. 21/49: loss=18.002140658854955, w0=70.93749204765392, w1=13.673455512767537\n",
      "SGD iter. 22/49: loss=9.59406569294269, w0=70.97715100281037, w1=13.611029855873355\n",
      "SGD iter. 23/49: loss=11.130941493679908, w0=71.2379767425081, w1=13.570286496203927\n",
      "SGD iter. 24/49: loss=11.345271061266029, w0=71.14621035567906, w1=13.782004529528503\n",
      "SGD iter. 25/49: loss=37.77965603451952, w0=71.52552989139723, w1=13.879737098595795\n",
      "SGD iter. 26/49: loss=20.863945068311224, w0=71.78638682360365, w1=13.44645585085416\n",
      "SGD iter. 27/49: loss=15.680904398273222, w0=71.70444952063508, w1=13.474048563852671\n",
      "SGD iter. 28/49: loss=12.401192228457976, w0=71.97929279110835, w1=13.495362477326992\n",
      "SGD iter. 29/49: loss=10.939666880857013, w0=71.86005254010837, w1=13.820699371762517\n",
      "SGD iter. 30/49: loss=34.58161213368156, w0=72.17360202209927, w1=13.85151527261675\n",
      "SGD iter. 31/49: loss=9.1068216116899, w0=71.99206659123446, w1=13.807972639649808\n",
      "SGD iter. 32/49: loss=12.286471263857013, w0=72.10014010912391, w1=13.913241794484243\n",
      "SGD iter. 33/49: loss=15.53528044080532, w0=71.79374512350978, w1=14.033288407429298\n",
      "SGD iter. 34/49: loss=24.64551117710723, w0=72.23037662819837, w1=13.76833255600713\n",
      "SGD iter. 35/49: loss=17.707838099502904, w0=72.58798571098555, w1=13.745751261288214\n",
      "SGD iter. 36/49: loss=27.49587015130836, w0=72.40475957154737, w1=13.62171942458538\n",
      "SGD iter. 37/49: loss=24.58007248755414, w0=72.67270955674084, w1=13.744946567076216\n",
      "SGD iter. 38/49: loss=12.036746900705115, w0=72.84311473069545, w1=13.572726138448866\n",
      "SGD iter. 39/49: loss=13.88890463007752, w0=73.27550800822827, w1=13.517104000852742\n",
      "SGD iter. 40/49: loss=14.648584809482031, w0=73.35188698537956, w1=13.330497100613021\n",
      "SGD iter. 41/49: loss=14.896031450373608, w0=73.1717331990811, w1=13.249411149861192\n",
      "SGD iter. 42/49: loss=19.076539186031585, w0=73.36553100630299, w1=13.33861397451793\n",
      "SGD iter. 43/49: loss=12.30644832253643, w0=73.49476465516082, w1=13.167201506737184\n",
      "SGD iter. 44/49: loss=11.12337753577616, w0=73.3461160247156, w1=13.122789152959808\n",
      "SGD iter. 45/49: loss=17.373511464275527, w0=73.2468535190783, w1=13.013494187354231\n",
      "SGD iter. 46/49: loss=19.098305476883784, w0=73.45392379071706, w1=12.704730501369987\n",
      "SGD iter. 47/49: loss=27.62717566382244, w0=73.91009633898734, w1=12.721676043827596\n",
      "SGD iter. 48/49: loss=19.41396314158873, w0=73.6632041645198, w1=12.788054719007981\n",
      "SGD iter. 49/49: loss=5.331627239989205, w0=73.64382828395965, w1=12.937093419889948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([223.5743533174922,\n",
       "  303.622048942763,\n",
       "  213.56737347054724,\n",
       "  155.07427162720347,\n",
       "  99.04187652752407,\n",
       "  133.891306164201,\n",
       "  76.93243639227562,\n",
       "  105.82740973404722,\n",
       "  107.2717077835191,\n",
       "  79.20818375567623,\n",
       "  48.07324512160052,\n",
       "  29.550816156808388,\n",
       "  26.64181823498439,\n",
       "  21.30313741688057,\n",
       "  45.456181896959265,\n",
       "  13.70052445454707,\n",
       "  14.028511049303711,\n",
       "  27.873902079152696,\n",
       "  14.854640939476184,\n",
       "  20.333464490473233,\n",
       "  9.985598457930028,\n",
       "  18.002140658854955,\n",
       "  9.59406569294269,\n",
       "  11.130941493679908,\n",
       "  11.345271061266029,\n",
       "  37.77965603451952,\n",
       "  20.863945068311224,\n",
       "  15.680904398273222,\n",
       "  12.401192228457976,\n",
       "  10.939666880857013,\n",
       "  34.58161213368156,\n",
       "  9.1068216116899,\n",
       "  12.286471263857013,\n",
       "  15.53528044080532,\n",
       "  24.64551117710723,\n",
       "  17.707838099502904,\n",
       "  27.49587015130836,\n",
       "  24.58007248755414,\n",
       "  12.036746900705115,\n",
       "  13.88890463007752,\n",
       "  14.648584809482031,\n",
       "  14.896031450373608,\n",
       "  19.076539186031585,\n",
       "  12.30644832253643,\n",
       "  11.12337753577616,\n",
       "  17.373511464275527,\n",
       "  19.098305476883784,\n",
       "  27.62717566382244,\n",
       "  19.41396314158873,\n",
       "  5.331627239989205],\n",
       " [array([50, 10]),\n",
       "  array([52.0839805 ,  9.65001612]),\n",
       "  array([54.48964811, 10.4311326 ]),\n",
       "  array([56.43905924, 10.2468728 ]),\n",
       "  array([58.07037704,  9.64069359]),\n",
       "  array([59.26783485,  9.93031574]),\n",
       "  array([60.77285779, 10.91545916]),\n",
       "  array([61.83877991, 11.31503756]),\n",
       "  array([63.21598682, 11.35338263]),\n",
       "  array([64.54305942, 11.81052643]),\n",
       "  array([65.46842676, 12.1791433 ]),\n",
       "  array([66.10764353, 12.41235245]),\n",
       "  array([66.76814776, 12.63301685]),\n",
       "  array([67.35790809, 12.71141156]),\n",
       "  array([67.80923026, 13.00289907]),\n",
       "  array([68.55424941, 13.38290062]),\n",
       "  array([68.95036401, 13.54389697]),\n",
       "  array([69.30975562, 13.55564583]),\n",
       "  array([69.89207094, 13.37048819]),\n",
       "  array([70.18638873, 13.67731813]),\n",
       "  array([70.46223955, 13.50952869]),\n",
       "  array([70.61048972, 13.55069064]),\n",
       "  array([70.93749205, 13.67345551]),\n",
       "  array([70.977151  , 13.61102986]),\n",
       "  array([71.23797674, 13.5702865 ]),\n",
       "  array([71.14621036, 13.78200453]),\n",
       "  array([71.52552989, 13.8797371 ]),\n",
       "  array([71.78638682, 13.44645585]),\n",
       "  array([71.70444952, 13.47404856]),\n",
       "  array([71.97929279, 13.49536248]),\n",
       "  array([71.86005254, 13.82069937]),\n",
       "  array([72.17360202, 13.85151527]),\n",
       "  array([71.99206659, 13.80797264]),\n",
       "  array([72.10014011, 13.91324179]),\n",
       "  array([71.79374512, 14.03328841]),\n",
       "  array([72.23037663, 13.76833256]),\n",
       "  array([72.58798571, 13.74575126]),\n",
       "  array([72.40475957, 13.62171942]),\n",
       "  array([72.67270956, 13.74494657]),\n",
       "  array([72.84311473, 13.57272614]),\n",
       "  array([73.27550801, 13.517104  ]),\n",
       "  array([73.35188699, 13.3304971 ]),\n",
       "  array([73.1717332 , 13.24941115]),\n",
       "  array([73.36553101, 13.33861397]),\n",
       "  array([73.49476466, 13.16720151]),\n",
       "  array([73.34611602, 13.12278915]),\n",
       "  array([73.24685352, 13.01349419]),\n",
       "  array([73.45392379, 12.7047305 ]),\n",
       "  array([73.91009634, 12.72167604]),\n",
       "  array([73.66320416, 12.78805472]),\n",
       "  array([73.64382828, 12.93709342])])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "index = np.random.randint(0,len(y),batch_size)\n",
    "y[index].shape\n",
    "stochastic_gradient_descent(y, tx, w_initial, batch_size, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2025.1193340043885, w0=6.364148543213599, w1=-0.9493434738837938\n",
      "SGD iter. 1/49: loss=2872.3374094146893, w0=13.94351185343259, w1=0.23804556869924198\n",
      "SGD iter. 2/49: loss=1982.1779744649361, w0=20.23982501982598, w1=0.33081803745124305\n",
      "SGD iter. 3/49: loss=1167.805395712214, w0=25.072640753354857, w1=1.9839655807805807\n",
      "SGD iter. 4/49: loss=1289.6035608354434, w0=30.151230244218326, w1=4.8376653338381095\n",
      "SGD iter. 5/49: loss=685.5192911871108, w0=33.85398406046674, w1=3.160930348802527\n",
      "SGD iter. 6/49: loss=356.4889934700998, w0=36.52414882493391, w1=2.0209554771941827\n",
      "SGD iter. 7/49: loss=1233.5362027757344, w0=41.4911120878859, w1=5.373061073653652\n",
      "SGD iter. 8/49: loss=211.1616686800254, w0=43.546162786433385, w1=4.337592823957799\n",
      "SGD iter. 9/49: loss=94.22841996857332, w0=44.91895861953147, w1=2.127944637290103\n",
      "SGD iter. 10/49: loss=206.40724224476585, w0=46.95074228047949, w1=0.857319519388478\n",
      "SGD iter. 11/49: loss=743.2224650791757, w0=50.80618639258687, w1=5.383387293482551\n",
      "SGD iter. 12/49: loss=202.56546957938403, w0=52.818972867001804, w1=6.284990968348639\n",
      "SGD iter. 13/49: loss=715.4951701763731, w0=56.601816163191515, w1=11.878710726657122\n",
      "SGD iter. 14/49: loss=163.03471024862418, w0=58.40755540377208, w1=11.20599890885509\n",
      "SGD iter. 15/49: loss=349.08627806459225, w0=61.04985091360663, w1=13.067976125499206\n",
      "SGD iter. 16/49: loss=87.34260353917959, w0=62.37153622818934, w1=12.20368182833177\n",
      "SGD iter. 17/49: loss=5.161938577552048, w0=62.692844135890105, w1=12.069719991197827\n",
      "SGD iter. 18/49: loss=286.53478016064685, w0=65.08673126840344, w1=15.554100352856741\n",
      "SGD iter. 19/49: loss=57.87752668238444, w0=66.16262649269235, w1=14.521596660288994\n",
      "SGD iter. 20/49: loss=132.96460611875503, w0=67.79336010747286, w1=12.38746821435457\n",
      "SGD iter. 21/49: loss=154.03136899725348, w0=69.54853171730053, w1=13.353947935775356\n",
      "SGD iter. 22/49: loss=33.86006505649909, w0=70.37145413231302, w1=13.011504818825568\n",
      "SGD iter. 23/49: loss=139.12770714695225, w0=72.03955309308814, w1=14.341041605171453\n",
      "SGD iter. 24/49: loss=120.4776707783861, w0=73.59182672088954, w1=15.767560766497015\n",
      "SGD iter. 25/49: loss=11.929621405222916, w0=74.08028595982813, w1=16.103816295106025\n",
      "SGD iter. 26/49: loss=17.185025244218757, w0=74.66654571871013, w1=15.441509336856397\n",
      "SGD iter. 27/49: loss=0.773357313403772, w0=74.54217873648383, w1=15.483990747956385\n",
      "SGD iter. 28/49: loss=0.09695974374463891, w0=74.58621502691449, w1=15.54215114101206\n",
      "SGD iter. 29/49: loss=0.07159417684556402, w0=74.5483747899053, w1=15.521284231713837\n",
      "SGD iter. 30/49: loss=63.14897256881466, w0=73.42455120438295, w1=13.190021040549489\n",
      "SGD iter. 31/49: loss=77.33508615043955, w0=72.18088656992204, w1=14.365093424465666\n",
      "SGD iter. 32/49: loss=12.764244589087548, w0=72.68614382299393, w1=14.715993020075983\n",
      "SGD iter. 33/49: loss=49.39517540229368, w0=71.6922104710805, w1=15.006921256401272\n",
      "SGD iter. 34/49: loss=14.654929345602897, w0=72.23359627336207, w1=15.088948167757248\n",
      "SGD iter. 35/49: loss=0.7118567028582318, w0=72.11427681117269, w1=15.15313745353522\n",
      "SGD iter. 36/49: loss=1.4786878423960867, w0=71.94230659035547, w1=14.917668279896128\n",
      "SGD iter. 37/49: loss=45.70970531305078, w0=70.98617160707067, w1=14.48769872320803\n",
      "SGD iter. 38/49: loss=45.0241172990838, w0=71.93510908972254, w1=15.09307652173267\n",
      "SGD iter. 39/49: loss=0.7919545830767305, w0=72.06096254283979, w1=14.932937343066866\n",
      "SGD iter. 40/49: loss=123.9260309161285, w0=73.6352943434214, w1=14.04613548961612\n",
      "SGD iter. 41/49: loss=46.72542630431793, w0=72.66859453164614, w1=13.010549654360421\n",
      "SGD iter. 42/49: loss=2.0198977288524493, w0=72.86958695582439, w1=12.711701806436414\n",
      "SGD iter. 43/49: loss=6.348409347512652, w0=72.51326099978097, w1=12.935932979948252\n",
      "SGD iter. 44/49: loss=61.33295914921132, w0=73.62080746967929, w1=12.890465509586212\n",
      "SGD iter. 45/49: loss=16.166504321568258, w0=73.0521862544302, w1=13.12503856844836\n",
      "SGD iter. 46/49: loss=10.754954380148332, w0=73.51597401580706, w1=13.070479959965345\n",
      "SGD iter. 47/49: loss=16.580884006340995, w0=72.94011146265152, w1=13.087101108772037\n",
      "SGD iter. 48/49: loss=0.6538250465684459, w0=73.0544639918381, w1=13.277453517706299\n",
      "SGD iter. 49/49: loss=20.50508464556764, w0=72.41407216419556, w1=12.766561322793319\n",
      "SGD: execution time=0.021 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7114f4dccdd40acb6f27828e0ded4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "# raise NotImplementedError\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.724426406192425\n",
      "GD iter. 1/49: loss=318.2821247015965, w0=67.40170332798297, w1=10.041754328050116\n",
      "GD iter. 2/49: loss=88.6423556165128, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.97477639885521, w0=73.46785662750146, w1=10.9455122175746\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.0516072257859, w1=11.032481534481912\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536945\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260336, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260336, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988166\n",
      "GD iter. 25/49: loss=65.93073010260336, w0=74.06780585492449, w1=11.034894865988823\n",
      "GD iter. 26/49: loss=65.93073010260336, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.005 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "# raise NotImplementedError\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc11302c70d44dcdabfce04ea9485f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    res = np.zeros((tx.shape[0],tx.shape[1]))\n",
    "    res[(y-tx.dot(w))>=0] = -tx[(y-tx.dot(w))>=0]\n",
    "    res[(y-tx.dot(w))< 0] = tx[(y-tx.dot(w))<0]\n",
    "    grid = np.mean(res,axis=0)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.zeros((tx.shape[0],tx.shape[1]))\n",
    "res.shape\n",
    "res[(y-tx.dot(w))>=0] = -tx[(y-tx.dot(w))>=0]\n",
    "res[(y-tx.dot(w))< 0] = tx[(y-tx.dot(w))<0]\n",
    "grid = np.mean(res,axis=0)\n",
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        loss = np.mean(np.abs(y-tx.dot(w)))\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        grid = compute_subgradient_mae(y,tx,w)\n",
    "        w = w - gamma*grid\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=73.29392200210518, w0=0.7, w1=-1.5529755259535704e-15\n",
      "SubGD iter. 1/499: loss=72.59392200210517, w0=1.4, w1=-3.1059510519071408e-15\n",
      "SubGD iter. 2/499: loss=71.89392200210517, w0=2.0999999999999996, w1=-4.658926577860711e-15\n",
      "SubGD iter. 3/499: loss=71.19392200210518, w0=2.8, w1=-6.2119021038142816e-15\n",
      "SubGD iter. 4/499: loss=70.49392200210517, w0=3.5, w1=-7.764877629767851e-15\n",
      "SubGD iter. 5/499: loss=69.79392200210518, w0=4.2, w1=-9.317853155721422e-15\n",
      "SubGD iter. 6/499: loss=69.09392200210519, w0=4.9, w1=-1.0870828681674993e-14\n",
      "SubGD iter. 7/499: loss=68.39392200210517, w0=5.6000000000000005, w1=-1.2423804207628563e-14\n",
      "SubGD iter. 8/499: loss=67.69392200210518, w0=6.300000000000001, w1=-1.3976779733582134e-14\n",
      "SubGD iter. 9/499: loss=66.99392200210517, w0=7.000000000000001, w1=-1.5529755259535703e-14\n",
      "SubGD iter. 10/499: loss=66.29392200210518, w0=7.700000000000001, w1=-1.7082730785489272e-14\n",
      "SubGD iter. 11/499: loss=65.59392200210519, w0=8.4, w1=-1.863570631144284e-14\n",
      "SubGD iter. 12/499: loss=64.89392200210517, w0=9.1, w1=-2.018868183739641e-14\n",
      "SubGD iter. 13/499: loss=64.19392200210517, w0=9.799999999999999, w1=-2.174165736334998e-14\n",
      "SubGD iter. 14/499: loss=63.49392200210517, w0=10.499999999999998, w1=-2.3294632889303548e-14\n",
      "SubGD iter. 15/499: loss=62.79392200210517, w0=11.199999999999998, w1=-2.4847608415257117e-14\n",
      "SubGD iter. 16/499: loss=62.09392200210518, w0=11.899999999999997, w1=-2.6400583941210686e-14\n",
      "SubGD iter. 17/499: loss=61.393922002105185, w0=12.599999999999996, w1=-2.7953559467164255e-14\n",
      "SubGD iter. 18/499: loss=60.69392200210517, w0=13.299999999999995, w1=-2.9506534993117824e-14\n",
      "SubGD iter. 19/499: loss=59.99392200210518, w0=13.999999999999995, w1=-3.105951051907139e-14\n",
      "SubGD iter. 20/499: loss=59.293922002105184, w0=14.699999999999994, w1=-3.261248604502496e-14\n",
      "SubGD iter. 21/499: loss=58.59392200210518, w0=15.399999999999993, w1=-3.416546157097853e-14\n",
      "SubGD iter. 22/499: loss=57.893922002105185, w0=16.099999999999994, w1=-3.57184370969321e-14\n",
      "SubGD iter. 23/499: loss=57.19392200210518, w0=16.799999999999994, w1=-3.727141262288567e-14\n",
      "SubGD iter. 24/499: loss=56.49392200210518, w0=17.499999999999993, w1=-3.882438814883924e-14\n",
      "SubGD iter. 25/499: loss=55.793922002105184, w0=18.199999999999992, w1=-4.0377363674792807e-14\n",
      "SubGD iter. 26/499: loss=55.09392200210518, w0=18.89999999999999, w1=-4.1930339200746376e-14\n",
      "SubGD iter. 27/499: loss=54.393922002105185, w0=19.59999999999999, w1=-4.3483314726699945e-14\n",
      "SubGD iter. 28/499: loss=53.69392200210518, w0=20.29999999999999, w1=-4.5036290252653514e-14\n",
      "SubGD iter. 29/499: loss=52.99392200210518, w0=20.99999999999999, w1=-4.658926577860708e-14\n",
      "SubGD iter. 30/499: loss=52.293922002105184, w0=21.69999999999999, w1=-4.814224130456065e-14\n",
      "SubGD iter. 31/499: loss=51.59392200210518, w0=22.399999999999988, w1=-4.969521683051422e-14\n",
      "SubGD iter. 32/499: loss=50.893922002105185, w0=23.099999999999987, w1=-5.124819235646779e-14\n",
      "SubGD iter. 33/499: loss=50.19392200210518, w0=23.799999999999986, w1=-5.280116788242136e-14\n",
      "SubGD iter. 34/499: loss=49.49392200210518, w0=24.499999999999986, w1=-5.435414340837493e-14\n",
      "SubGD iter. 35/499: loss=48.79392200210519, w0=25.199999999999985, w1=-5.59071189343285e-14\n",
      "SubGD iter. 36/499: loss=48.09392200210519, w0=25.899999999999984, w1=-5.746009446028207e-14\n",
      "SubGD iter. 37/499: loss=47.393922002105185, w0=26.599999999999984, w1=-5.901306998623565e-14\n",
      "SubGD iter. 38/499: loss=46.6939220021052, w0=27.299999999999983, w1=-6.056604551218922e-14\n",
      "SubGD iter. 39/499: loss=45.99392200210519, w0=27.999999999999982, w1=-6.21190210381428e-14\n",
      "SubGD iter. 40/499: loss=45.29392200210519, w0=28.69999999999998, w1=-6.367199656409637e-14\n",
      "SubGD iter. 41/499: loss=44.593922002105195, w0=29.39999999999998, w1=-6.522497209004995e-14\n",
      "SubGD iter. 42/499: loss=43.89392723059967, w0=30.099859999999982, w1=0.0004404657702421764\n",
      "SubGD iter. 43/499: loss=43.194206925442394, w0=30.799719999999983, w1=0.0008809315405495777\n",
      "SubGD iter. 44/499: loss=42.494486620285116, w0=31.499579999999984, w1=0.001321397310856979\n",
      "SubGD iter. 45/499: loss=41.794801882440076, w0=32.19929999999999, w1=0.002151200058803724\n",
      "SubGD iter. 46/499: loss=41.09536078676493, w0=32.899019999999986, w1=0.002981002806750469\n",
      "SubGD iter. 47/499: loss=40.396015121762325, w0=33.59859999999998, w1=0.004238399708372432\n",
      "SubGD iter. 48/499: loss=39.696964631836686, w0=34.298039999999986, w1=0.005823822408791713\n",
      "SubGD iter. 49/499: loss=38.99808059302934, w0=34.99747999999999, w1=0.0074092451092109945\n",
      "SubGD iter. 50/499: loss=38.299196554222, w0=35.69691999999999, w1=0.008994667809630276\n",
      "SubGD iter. 51/499: loss=37.600470352601796, w0=36.39607999999999, w1=0.011248049853488248\n",
      "SubGD iter. 52/499: loss=36.90236166793368, w0=37.09495999999999, w1=0.014269124444697119\n",
      "SubGD iter. 53/499: loss=36.20468598163629, w0=37.793699999999994, w1=0.01766349946233946\n",
      "SubGD iter. 54/499: loss=35.507288901302104, w0=38.49215999999999, w1=0.021587972991156293\n",
      "SubGD iter. 55/499: loss=34.81047272559701, w0=39.19019999999999, w1=0.026549981650513393\n",
      "SubGD iter. 56/499: loss=34.11463938511051, w0=39.88739999999999, w1=0.0334590206309035\n",
      "SubGD iter. 57/499: loss=33.42034933030703, w0=40.584039999999995, w1=0.04155352991358485\n",
      "SubGD iter. 58/499: loss=32.72708260590542, w0=41.28025999999999, w1=0.05080738787870366\n",
      "SubGD iter. 59/499: loss=32.035274191106154, w0=41.97507999999999, w1=0.06316953459264504\n",
      "SubGD iter. 60/499: loss=31.346253501120472, w0=42.667939999999994, w1=0.07970663453187159\n",
      "SubGD iter. 61/499: loss=30.66081118573862, w0=43.35925999999999, w1=0.09929003796957596\n",
      "SubGD iter. 62/499: loss=29.97824373392408, w0=44.04889999999999, w1=0.12249682042556717\n",
      "SubGD iter. 63/499: loss=29.29919217002557, w0=44.735739999999986, w1=0.15117673515145405\n",
      "SubGD iter. 64/499: loss=28.625370150150545, w0=45.41991999999998, w1=0.1848471294408023\n",
      "SubGD iter. 65/499: loss=27.95640812788488, w0=46.10073999999998, w1=0.22490170464881898\n",
      "SubGD iter. 66/499: loss=27.293211335702274, w0=46.77847999999998, w1=0.27030807598900264\n",
      "SubGD iter. 67/499: loss=26.635946775492176, w0=47.45243999999998, w1=0.32205641313644706\n",
      "SubGD iter. 68/499: loss=25.986354455661576, w0=48.119959999999985, w1=0.3843602843516495\n",
      "SubGD iter. 69/499: loss=25.34635990054962, w0=48.78257999999998, w1=0.45460143187065083\n",
      "SubGD iter. 70/499: loss=24.713407766391825, w0=49.44141999999998, w1=0.5311991534378644\n",
      "SubGD iter. 71/499: loss=24.086931144489547, w0=50.094659999999976, w1=0.6167864550871748\n",
      "SubGD iter. 72/499: loss=23.469061835298344, w0=50.74159999999998, w1=0.7118294204257005\n",
      "SubGD iter. 73/499: loss=22.860312219449888, w0=51.382239999999975, w1=0.8163783816188553\n",
      "SubGD iter. 74/499: loss=22.260394194603613, w0=52.015879999999974, w1=0.9309819479180104\n",
      "SubGD iter. 75/499: loss=21.670214357989362, w0=52.64223999999997, w1=1.056041039445561\n",
      "SubGD iter. 76/499: loss=21.089713092061302, w0=53.261179999999975, w1=1.1909782249707368\n",
      "SubGD iter. 77/499: loss=20.51839520029387, w0=53.87353999999998, w1=1.3354326069268716\n",
      "SubGD iter. 78/499: loss=19.954842019021974, w0=54.479039999999976, w1=1.488859155423101\n",
      "SubGD iter. 79/499: loss=19.39981727796524, w0=55.07641999999998, w1=1.6526057998941344\n",
      "SubGD iter. 80/499: loss=18.854292127308746, w0=55.664699999999975, w1=1.827199911738537\n",
      "SubGD iter. 81/499: loss=18.31878394789317, w0=56.24345999999998, w1=2.013212915976214\n",
      "SubGD iter. 82/499: loss=17.792491479525168, w0=56.81479999999998, w1=2.2075986497860063\n",
      "SubGD iter. 83/499: loss=17.27474434540427, w0=57.37633999999998, w1=2.411376270974464\n",
      "SubGD iter. 84/499: loss=16.766906752678832, w0=57.92975999999998, w1=2.6230481013704767\n",
      "SubGD iter. 85/499: loss=16.267994479749298, w0=58.47407999999998, w1=2.8433267790651087\n",
      "SubGD iter. 86/499: loss=15.777334186667382, w0=59.00957999999998, w1=3.072330965520354\n",
      "SubGD iter. 87/499: loss=15.294906834780795, w0=59.53569999999998, w1=3.3102924945409007\n",
      "SubGD iter. 88/499: loss=14.820404573025089, w0=60.053559999999976, w1=3.555351748353824\n",
      "SubGD iter. 89/499: loss=14.354283714893946, w0=60.559939999999976, w1=3.809170691886062\n",
      "SubGD iter. 90/499: loss=13.898001392894923, w0=61.057499999999976, w1=4.069720751433983\n",
      "SubGD iter. 91/499: loss=13.44882382810378, w0=61.547919999999976, w1=4.335508393726905\n",
      "SubGD iter. 92/499: loss=13.006044196344101, w0=62.02965999999998, w1=4.6069897470010375\n",
      "SubGD iter. 93/499: loss=12.57110174936068, w0=62.502719999999975, w1=4.88502014251682\n",
      "SubGD iter. 94/499: loss=12.143380485959392, w0=62.96583999999997, w1=5.16875462665388\n",
      "SubGD iter. 95/499: loss=11.723894837245583, w0=63.41971999999997, w1=5.456484559032046\n",
      "SubGD iter. 96/499: loss=11.313435023601551, w0=63.86463999999997, w1=5.747997496770113\n",
      "SubGD iter. 97/499: loss=10.911156920015099, w0=64.30129999999997, w1=6.043221223968383\n",
      "SubGD iter. 98/499: loss=10.516948122608078, w0=64.72773999999997, w1=6.340354080836923\n",
      "SubGD iter. 99/499: loss=10.13359513103867, w0=65.14381999999996, w1=6.63994524590634\n",
      "SubGD iter. 100/499: loss=9.760677429923486, w0=65.54995999999996, w1=6.939754306256994\n",
      "SubGD iter. 101/499: loss=9.398674623189937, w0=65.94755999999995, w1=7.240382293155359\n",
      "SubGD iter. 102/499: loss=9.046552477133655, w0=66.33451999999996, w1=7.542759507922872\n",
      "SubGD iter. 103/499: loss=8.704760514732415, w0=66.71139999999995, w1=7.846017516367578\n",
      "SubGD iter. 104/499: loss=8.372871671628266, w0=67.07791999999995, w1=8.149168076773346\n",
      "SubGD iter. 105/499: loss=8.052713872262597, w0=67.43603999999995, w1=8.450471378640396\n",
      "SubGD iter. 106/499: loss=7.741845358498816, w0=67.78645999999995, w1=8.751045452661094\n",
      "SubGD iter. 107/499: loss=7.440636497847948, w0=68.12511999999995, w1=9.04921572225288\n",
      "SubGD iter. 108/499: loss=7.152144892432378, w0=68.45537999999995, w1=9.343976087699637\n",
      "SubGD iter. 109/499: loss=6.874953545674702, w0=68.77695999999995, w1=9.635092372673002\n",
      "SubGD iter. 110/499: loss=6.609759691657253, w0=69.08803999999995, w1=9.919493282073365\n",
      "SubGD iter. 111/499: loss=6.359664465347405, w0=69.38749999999995, w1=10.197623582279002\n",
      "SubGD iter. 112/499: loss=6.125559830337084, w0=69.67281999999994, w1=10.46806359361339\n",
      "SubGD iter. 113/499: loss=5.908756851126104, w0=69.94651999999995, w1=10.728826346302005\n",
      "SubGD iter. 114/499: loss=5.708346277966668, w0=70.20887999999995, w1=10.979299692022279\n",
      "SubGD iter. 115/499: loss=5.524903595719347, w0=70.45653999999995, w1=11.220392679253374\n",
      "SubGD iter. 116/499: loss=5.358572110476778, w0=70.69215999999994, w1=11.449066352016724\n",
      "SubGD iter. 117/499: loss=5.208366578733325, w0=70.91545999999994, w1=11.66451264164341\n",
      "SubGD iter. 118/499: loss=5.075452050918035, w0=71.12447999999993, w1=11.864387168308943\n",
      "SubGD iter. 119/499: loss=4.960839914882402, w0=71.31865999999994, w1=12.045105403559484\n",
      "SubGD iter. 120/499: loss=4.864240181709682, w0=71.49939999999994, w1=12.20983607316598\n",
      "SubGD iter. 121/499: loss=4.782724768945261, w0=71.66543999999993, w1=12.357476521898722\n",
      "SubGD iter. 122/499: loss=4.715208074668135, w0=71.81593999999993, w1=12.48980888194029\n",
      "SubGD iter. 123/499: loss=4.660146063774253, w0=71.95369999999993, w1=12.612418157319299\n",
      "SubGD iter. 124/499: loss=4.6138484530642225, w0=72.08067999999993, w1=12.721366944472148\n",
      "SubGD iter. 125/499: loss=4.576038370591764, w0=72.19435999999993, w1=12.815458765817613\n",
      "SubGD iter. 126/499: loss=4.5466498510490405, w0=72.29697999999993, w1=12.90059276939534\n",
      "SubGD iter. 127/499: loss=4.522563544988974, w0=72.38965999999994, w1=12.97576965427992\n",
      "SubGD iter. 128/499: loss=4.503077398110059, w0=72.47435999999993, w1=13.04396334930227\n",
      "SubGD iter. 129/499: loss=4.487012427842286, w0=72.55093999999994, w1=13.103277584601688\n",
      "SubGD iter. 130/499: loss=4.474309393425119, w0=72.62051999999994, w1=13.156032035188144\n",
      "SubGD iter. 131/499: loss=4.464166669156606, w0=72.68239999999994, w1=13.201870566969161\n",
      "SubGD iter. 132/499: loss=4.456202124936147, w0=72.73769999999995, w1=13.241365939188695\n",
      "SubGD iter. 133/499: loss=4.449913837301208, w0=72.78823999999994, w1=13.276636749385748\n",
      "SubGD iter. 134/499: loss=4.4447394482327685, w0=72.83401999999994, w1=13.308112586749687\n",
      "SubGD iter. 135/499: loss=4.440577063150134, w0=72.87447999999993, w1=13.335151520405006\n",
      "SubGD iter. 136/499: loss=4.4373967610583085, w0=72.91115999999994, w1=13.358430829065458\n",
      "SubGD iter. 137/499: loss=4.4348465287075, w0=72.94447999999994, w1=13.379256780851263\n",
      "SubGD iter. 138/499: loss=4.432811551436954, w0=72.97401999999994, w1=13.396406376905544\n",
      "SubGD iter. 139/499: loss=4.431233661858322, w0=73.00103999999993, w1=13.41218912045937\n",
      "SubGD iter. 140/499: loss=4.429913776135237, w0=73.02483999999993, w1=13.425274895313084\n",
      "SubGD iter. 141/499: loss=4.428916084676415, w0=73.04625999999993, w1=13.436222021322303\n",
      "SubGD iter. 142/499: loss=4.42811221458972, w0=73.06613999999993, w1=13.447440046601253\n",
      "SubGD iter. 143/499: loss=4.427409042227934, w0=73.08405999999994, w1=13.457856669597893\n",
      "SubGD iter. 144/499: loss=4.426825784996949, w0=73.10029999999993, w1=13.467335162734873\n",
      "SubGD iter. 145/499: loss=4.426369603365169, w0=73.11401999999994, w1=13.475393628220223\n",
      "SubGD iter. 146/499: loss=4.426030178383453, w0=73.12619999999994, w1=13.482212864437892\n",
      "SubGD iter. 147/499: loss=4.425766799370374, w0=73.13697999999994, w1=13.48799725691146\n",
      "SubGD iter. 148/499: loss=4.425555929007912, w0=73.14747999999993, w1=13.493479439140252\n",
      "SubGD iter. 149/499: loss=4.425362490640805, w0=73.15699999999993, w1=13.498468271254364\n",
      "SubGD iter. 150/499: loss=4.425203617363585, w0=73.16595999999993, w1=13.50294680429713\n",
      "SubGD iter. 151/499: loss=4.425068527947261, w0=73.17365999999993, w1=13.506576029236252\n",
      "SubGD iter. 152/499: loss=4.424972782434856, w0=73.18009999999992, w1=13.509768665455107\n",
      "SubGD iter. 153/499: loss=4.424903121175712, w0=73.18597999999993, w1=13.512571468366584\n",
      "SubGD iter. 154/499: loss=4.424846327975846, w0=73.19143999999993, w1=13.515117410468855\n",
      "SubGD iter. 155/499: loss=4.42479923253376, w0=73.19605999999993, w1=13.517185961787685\n",
      "SubGD iter. 156/499: loss=4.424766409404, w0=73.19969999999994, w1=13.518069930874283\n",
      "SubGD iter. 157/499: loss=4.424747663519319, w0=73.20291999999993, w1=13.518854137767013\n",
      "SubGD iter. 158/499: loss=4.424732182668888, w0=73.20599999999993, w1=13.51952983926887\n",
      "SubGD iter. 159/499: loss=4.424717978422431, w0=73.20907999999993, w1=13.520205540770727\n",
      "SubGD iter. 160/499: loss=4.424704053977274, w0=73.21201999999992, w1=13.521000216270515\n",
      "SubGD iter. 161/499: loss=4.4246908038213455, w0=73.21495999999992, w1=13.521794891770304\n",
      "SubGD iter. 162/499: loss=4.424678207991666, w0=73.21747999999992, w1=13.522407209546943\n",
      "SubGD iter. 163/499: loss=4.424668911605651, w0=73.21985999999993, w1=13.522732908460886\n",
      "SubGD iter. 164/499: loss=4.424660947850395, w0=73.22181999999992, w1=13.522991164028737\n",
      "SubGD iter. 165/499: loss=4.424655537307837, w0=73.22363999999992, w1=13.523086524955808\n",
      "SubGD iter. 166/499: loss=4.424651183353665, w0=73.22503999999992, w1=13.522757010157395\n",
      "SubGD iter. 167/499: loss=4.424648228239375, w0=73.22643999999993, w1=13.522427495358983\n",
      "SubGD iter. 168/499: loss=4.424645273125086, w0=73.22783999999993, w1=13.52209798056057\n",
      "SubGD iter. 169/499: loss=4.424642498283796, w0=73.22895999999993, w1=13.522225963141242\n",
      "SubGD iter. 170/499: loss=4.424640682884452, w0=73.23007999999993, w1=13.522353945721914\n",
      "SubGD iter. 171/499: loss=4.424638867485107, w0=73.23119999999993, w1=13.522481928302586\n",
      "SubGD iter. 172/499: loss=4.424637300852184, w0=73.23189999999992, w1=13.523013495747021\n",
      "SubGD iter. 173/499: loss=4.424636258655271, w0=73.23273999999992, w1=13.52326627956579\n",
      "SubGD iter. 174/499: loss=4.424635339412463, w0=73.23329999999991, w1=13.523741235074766\n",
      "SubGD iter. 175/499: loss=4.424634640448436, w0=73.23399999999991, w1=13.523937406958076\n",
      "SubGD iter. 176/499: loss=4.424633962890472, w0=73.2345599999999, w1=13.523888604707691\n",
      "SubGD iter. 177/499: loss=4.424633633499305, w0=73.23497999999991, w1=13.524118586082972\n",
      "SubGD iter. 178/499: loss=4.424633305940116, w0=73.23539999999991, w1=13.524348567458253\n",
      "SubGD iter. 179/499: loss=4.424632978380924, w0=73.23581999999992, w1=13.524578548833533\n",
      "SubGD iter. 180/499: loss=4.424632650821734, w0=73.23623999999992, w1=13.524808530208814\n",
      "SubGD iter. 181/499: loss=4.424632389924093, w0=73.23651999999993, w1=13.524964267899264\n",
      "SubGD iter. 182/499: loss=4.424632243275195, w0=73.23679999999993, w1=13.525120005589715\n",
      "SubGD iter. 183/499: loss=4.424632096626298, w0=73.23707999999993, w1=13.525275743280165\n",
      "SubGD iter. 184/499: loss=4.424631993328702, w0=73.23721999999994, w1=13.525174424196196\n",
      "SubGD iter. 185/499: loss=4.424631950663619, w0=73.23735999999994, w1=13.525073105112227\n",
      "SubGD iter. 186/499: loss=4.424631907998538, w0=73.23749999999994, w1=13.524971786028258\n",
      "SubGD iter. 187/499: loss=4.424631865333457, w0=73.23763999999994, w1=13.52487046694429\n",
      "SubGD iter. 188/499: loss=4.424631822668376, w0=73.23777999999994, w1=13.52476914786032\n",
      "SubGD iter. 189/499: loss=4.4246317826859745, w0=73.23805999999995, w1=13.52492488555077\n",
      "SubGD iter. 190/499: loss=4.42463174654501, w0=73.23819999999995, w1=13.524823566466802\n",
      "SubGD iter. 191/499: loss=4.424631703879928, w0=73.23833999999995, w1=13.524722247382833\n",
      "SubGD iter. 192/499: loss=4.424631661214848, w0=73.23847999999995, w1=13.524620928298864\n",
      "SubGD iter. 193/499: loss=4.424631622214842, w0=73.23847999999995, w1=13.524586781150308\n",
      "SubGD iter. 194/499: loss=4.424631620549089, w0=73.23847999999995, w1=13.524552634001752\n",
      "SubGD iter. 195/499: loss=4.424631618883335, w0=73.23847999999995, w1=13.524518486853196\n",
      "SubGD iter. 196/499: loss=4.424631617217582, w0=73.23847999999995, w1=13.52448433970464\n",
      "SubGD iter. 197/499: loss=4.424631615551827, w0=73.23847999999995, w1=13.524450192556085\n",
      "SubGD iter. 198/499: loss=4.424631613886072, w0=73.23847999999995, w1=13.524416045407529\n",
      "SubGD iter. 199/499: loss=4.4246316122203195, w0=73.23847999999995, w1=13.524381898258973\n",
      "SubGD iter. 200/499: loss=4.424631615444577, w0=73.23861999999995, w1=13.524604807884836\n",
      "SubGD iter. 201/499: loss=4.424631621428463, w0=73.23861999999995, w1=13.52457066073628\n",
      "SubGD iter. 202/499: loss=4.424631619762709, w0=73.23861999999995, w1=13.524536513587725\n",
      "SubGD iter. 203/499: loss=4.424631618096955, w0=73.23861999999995, w1=13.524502366439169\n",
      "SubGD iter. 204/499: loss=4.424631616431201, w0=73.23861999999995, w1=13.524468219290613\n",
      "SubGD iter. 205/499: loss=4.424631614765448, w0=73.23861999999995, w1=13.524434072142057\n",
      "SubGD iter. 206/499: loss=4.4246316130996926, w0=73.23861999999995, w1=13.524399924993501\n",
      "SubGD iter. 207/499: loss=4.42463161143394, w0=73.23861999999995, w1=13.524365777844945\n",
      "SubGD iter. 208/499: loss=4.424631609768185, w0=73.23861999999995, w1=13.52433163069639\n",
      "SubGD iter. 209/499: loss=4.42463160810243, w0=73.23861999999995, w1=13.524297483547834\n",
      "SubGD iter. 210/499: loss=4.4246316143257935, w0=73.23875999999996, w1=13.524520393173697\n",
      "SubGD iter. 211/499: loss=4.424631617310575, w0=73.23875999999996, w1=13.524486246025141\n",
      "SubGD iter. 212/499: loss=4.424631615644821, w0=73.23875999999996, w1=13.524452098876585\n",
      "SubGD iter. 213/499: loss=4.4246316139790665, w0=73.23875999999996, w1=13.52441795172803\n",
      "SubGD iter. 214/499: loss=4.424631612313314, w0=73.23875999999996, w1=13.524383804579474\n",
      "SubGD iter. 215/499: loss=4.424631610647559, w0=73.23875999999996, w1=13.524349657430918\n",
      "SubGD iter. 216/499: loss=4.424631608981806, w0=73.23875999999996, w1=13.524315510282362\n",
      "SubGD iter. 217/499: loss=4.424631607316051, w0=73.23875999999996, w1=13.524281363133806\n",
      "SubGD iter. 218/499: loss=4.424631605650297, w0=73.23875999999996, w1=13.52424721598525\n",
      "SubGD iter. 219/499: loss=4.424631603984543, w0=73.23875999999996, w1=13.524213068836694\n",
      "SubGD iter. 220/499: loss=4.42463161320701, w0=73.23889999999996, w1=13.524435978462558\n",
      "SubGD iter. 221/499: loss=4.424631613192687, w0=73.23889999999996, w1=13.524401831314002\n",
      "SubGD iter. 222/499: loss=4.424631611526932, w0=73.23889999999996, w1=13.524367684165446\n",
      "SubGD iter. 223/499: loss=4.424631609861179, w0=73.23889999999996, w1=13.52433353701689\n",
      "SubGD iter. 224/499: loss=4.4246316081954244, w0=73.23889999999996, w1=13.524299389868334\n",
      "SubGD iter. 225/499: loss=4.424631606529671, w0=73.23889999999996, w1=13.524265242719778\n",
      "SubGD iter. 226/499: loss=4.424631604863916, w0=73.23889999999996, w1=13.524231095571222\n",
      "SubGD iter. 227/499: loss=4.424631603198163, w0=73.23889999999996, w1=13.524196948422667\n",
      "SubGD iter. 228/499: loss=4.4246316015324085, w0=73.23889999999996, w1=13.52416280127411\n",
      "SubGD iter. 229/499: loss=4.4246316012143305, w0=73.23903999999996, w1=13.524385710899974\n",
      "SubGD iter. 230/499: loss=4.424631610740552, w0=73.23903999999996, w1=13.524351563751418\n",
      "SubGD iter. 231/499: loss=4.424631609074798, w0=73.23903999999996, w1=13.524317416602862\n",
      "SubGD iter. 232/499: loss=4.424631607409045, w0=73.23903999999996, w1=13.524283269454306\n",
      "SubGD iter. 233/499: loss=4.42463160574329, w0=73.23903999999996, w1=13.52424912230575\n",
      "SubGD iter. 234/499: loss=4.424631604077537, w0=73.23903999999996, w1=13.524214975157195\n",
      "SubGD iter. 235/499: loss=4.424631602411783, w0=73.23903999999996, w1=13.524180828008639\n",
      "SubGD iter. 236/499: loss=4.42463160074603, w0=73.23903999999996, w1=13.524146680860083\n",
      "SubGD iter. 237/499: loss=4.424631599080274, w0=73.23903999999996, w1=13.524112533711527\n",
      "SubGD iter. 238/499: loss=4.424631597414521, w0=73.23903999999996, w1=13.524078386562971\n",
      "SubGD iter. 239/499: loss=4.424631600095546, w0=73.23917999999996, w1=13.524301296188835\n",
      "SubGD iter. 240/499: loss=4.424631606622665, w0=73.23917999999996, w1=13.524267149040279\n",
      "SubGD iter. 241/499: loss=4.42463160495691, w0=73.23917999999996, w1=13.524233001891723\n",
      "SubGD iter. 242/499: loss=4.4246316032911555, w0=73.23917999999996, w1=13.524198854743167\n",
      "SubGD iter. 243/499: loss=4.424631601625403, w0=73.23917999999996, w1=13.524164707594611\n",
      "SubGD iter. 244/499: loss=4.424631599959648, w0=73.23917999999996, w1=13.524130560446055\n",
      "SubGD iter. 245/499: loss=4.424631598293895, w0=73.23917999999996, w1=13.5240964132975\n",
      "SubGD iter. 246/499: loss=4.42463159662814, w0=73.23917999999996, w1=13.524062266148944\n",
      "SubGD iter. 247/499: loss=4.424631594962388, w0=73.23917999999996, w1=13.524028119000388\n",
      "SubGD iter. 248/499: loss=4.424631593296633, w0=73.23917999999996, w1=13.523993971851832\n",
      "SubGD iter. 249/499: loss=4.424631598976763, w0=73.23931999999996, w1=13.524216881477695\n",
      "SubGD iter. 250/499: loss=4.424631602504776, w0=73.23931999999996, w1=13.52418273432914\n",
      "SubGD iter. 251/499: loss=4.424631600839022, w0=73.23931999999996, w1=13.524148587180584\n",
      "SubGD iter. 252/499: loss=4.424631599173268, w0=73.23931999999996, w1=13.524114440032028\n",
      "SubGD iter. 253/499: loss=4.4246315975075134, w0=73.23931999999996, w1=13.524080292883472\n",
      "SubGD iter. 254/499: loss=4.424631595841761, w0=73.23931999999996, w1=13.524046145734916\n",
      "SubGD iter. 255/499: loss=4.424631594176007, w0=73.23931999999996, w1=13.52401199858636\n",
      "SubGD iter. 256/499: loss=4.424631592510252, w0=73.23931999999996, w1=13.523977851437804\n",
      "SubGD iter. 257/499: loss=4.424631590844498, w0=73.23931999999996, w1=13.523943704289248\n",
      "SubGD iter. 258/499: loss=4.424631589178745, w0=73.23931999999996, w1=13.523909557140692\n",
      "SubGD iter. 259/499: loss=4.42463159785798, w0=73.23945999999997, w1=13.524132466766556\n",
      "SubGD iter. 260/499: loss=4.424631598386888, w0=73.23945999999997, w1=13.524098319618\n",
      "SubGD iter. 261/499: loss=4.424631596721134, w0=73.23945999999997, w1=13.524064172469444\n",
      "SubGD iter. 262/499: loss=4.42463159505538, w0=73.23945999999997, w1=13.524030025320888\n",
      "SubGD iter. 263/499: loss=4.424631593389626, w0=73.23945999999997, w1=13.523995878172332\n",
      "SubGD iter. 264/499: loss=4.424631591723872, w0=73.23945999999997, w1=13.523961731023777\n",
      "SubGD iter. 265/499: loss=4.4246315900581195, w0=73.23945999999997, w1=13.52392758387522\n",
      "SubGD iter. 266/499: loss=4.424631588392364, w0=73.23945999999997, w1=13.523893436726665\n",
      "SubGD iter. 267/499: loss=4.42463158672661, w0=73.23945999999997, w1=13.523859289578109\n",
      "SubGD iter. 268/499: loss=4.424631585865299, w0=73.23959999999997, w1=13.524082199203972\n",
      "SubGD iter. 269/499: loss=4.424631595934754, w0=73.23959999999997, w1=13.524048052055416\n",
      "SubGD iter. 270/499: loss=4.424631594269001, w0=73.23959999999997, w1=13.52401390490686\n",
      "SubGD iter. 271/499: loss=4.424631592603246, w0=73.23959999999997, w1=13.523979757758305\n",
      "SubGD iter. 272/499: loss=4.424631590937492, w0=73.23959999999997, w1=13.523945610609749\n",
      "SubGD iter. 273/499: loss=4.424631589271739, w0=73.23959999999997, w1=13.523911463461193\n",
      "SubGD iter. 274/499: loss=4.424631587605984, w0=73.23959999999997, w1=13.523877316312637\n",
      "SubGD iter. 275/499: loss=4.424631585940229, w0=73.23959999999997, w1=13.523843169164081\n",
      "SubGD iter. 276/499: loss=4.424631584274477, w0=73.23959999999997, w1=13.523809022015525\n",
      "SubGD iter. 277/499: loss=4.424631582608722, w0=73.23959999999997, w1=13.52377487486697\n",
      "SubGD iter. 278/499: loss=4.424631584746517, w0=73.23973999999997, w1=13.523997784492833\n",
      "SubGD iter. 279/499: loss=4.424631591816865, w0=73.23973999999997, w1=13.523963637344277\n",
      "SubGD iter. 280/499: loss=4.424631590151113, w0=73.23973999999997, w1=13.523929490195721\n",
      "SubGD iter. 281/499: loss=4.424631588485357, w0=73.23973999999997, w1=13.523895343047165\n",
      "SubGD iter. 282/499: loss=4.424631586819605, w0=73.23973999999997, w1=13.52386119589861\n",
      "SubGD iter. 283/499: loss=4.4246315851538505, w0=73.23973999999997, w1=13.523827048750054\n",
      "SubGD iter. 284/499: loss=4.424631583488096, w0=73.23973999999997, w1=13.523792901601498\n",
      "SubGD iter. 285/499: loss=4.424631581822343, w0=73.23973999999997, w1=13.523758754452942\n",
      "SubGD iter. 286/499: loss=4.424631580156588, w0=73.23973999999997, w1=13.523724607304386\n",
      "SubGD iter. 287/499: loss=4.4246315784908345, w0=73.23973999999997, w1=13.52369046015583\n",
      "SubGD iter. 288/499: loss=4.424631583627733, w0=73.23987999999997, w1=13.523913369781694\n",
      "SubGD iter. 289/499: loss=4.424631587698978, w0=73.23987999999997, w1=13.523879222633138\n",
      "SubGD iter. 290/499: loss=4.4246315860332235, w0=73.23987999999997, w1=13.523845075484582\n",
      "SubGD iter. 291/499: loss=4.424631584367469, w0=73.23987999999997, w1=13.523810928336026\n",
      "SubGD iter. 292/499: loss=4.424631582701716, w0=73.23987999999997, w1=13.52377678118747\n",
      "SubGD iter. 293/499: loss=4.424631581035962, w0=73.23987999999997, w1=13.523742634038914\n",
      "SubGD iter. 294/499: loss=4.424631579370208, w0=73.23987999999997, w1=13.523708486890358\n",
      "SubGD iter. 295/499: loss=4.424631577704453, w0=73.23987999999997, w1=13.523674339741802\n",
      "SubGD iter. 296/499: loss=4.4246315760387, w0=73.23987999999997, w1=13.523640192593247\n",
      "SubGD iter. 297/499: loss=4.424631574372945, w0=73.23987999999997, w1=13.52360604544469\n",
      "SubGD iter. 298/499: loss=4.424631582508949, w0=73.24001999999997, w1=13.523828955070554\n",
      "SubGD iter. 299/499: loss=4.424631583581089, w0=73.24001999999997, w1=13.523794807921998\n",
      "SubGD iter. 300/499: loss=4.424631581915335, w0=73.24001999999997, w1=13.523760660773442\n",
      "SubGD iter. 301/499: loss=4.4246315802495815, w0=73.24001999999997, w1=13.523726513624887\n",
      "SubGD iter. 302/499: loss=4.424631578583828, w0=73.24001999999997, w1=13.52369236647633\n",
      "SubGD iter. 303/499: loss=4.424631576918073, w0=73.24001999999997, w1=13.523658219327775\n",
      "SubGD iter. 304/499: loss=4.42463157525232, w0=73.24001999999997, w1=13.523624072179219\n",
      "SubGD iter. 305/499: loss=4.4246315735865664, w0=73.24001999999997, w1=13.523589925030663\n",
      "SubGD iter. 306/499: loss=4.424631571920813, w0=73.24001999999997, w1=13.523555777882107\n",
      "SubGD iter. 307/499: loss=4.424631570516268, w0=73.24015999999997, w1=13.52377868750797\n",
      "SubGD iter. 308/499: loss=4.424631581128955, w0=73.24015999999997, w1=13.523744540359415\n",
      "SubGD iter. 309/499: loss=4.424631579463202, w0=73.24015999999997, w1=13.523710393210859\n",
      "SubGD iter. 310/499: loss=4.424631577797447, w0=73.24015999999997, w1=13.523676246062303\n",
      "SubGD iter. 311/499: loss=4.424631576131694, w0=73.24015999999997, w1=13.523642098913747\n",
      "SubGD iter. 312/499: loss=4.4246315744659395, w0=73.24015999999997, w1=13.523607951765191\n",
      "SubGD iter. 313/499: loss=4.424631572800186, w0=73.24015999999997, w1=13.523573804616635\n",
      "SubGD iter. 314/499: loss=4.424631571134432, w0=73.24015999999997, w1=13.52353965746808\n",
      "SubGD iter. 315/499: loss=4.424631569468677, w0=73.24015999999997, w1=13.523505510319524\n",
      "SubGD iter. 316/499: loss=4.424631567802924, w0=73.24015999999997, w1=13.523471363170968\n",
      "SubGD iter. 317/499: loss=4.424631569397486, w0=73.24029999999998, w1=13.523694272796831\n",
      "SubGD iter. 318/499: loss=4.424631577011067, w0=73.24029999999998, w1=13.523660125648275\n",
      "SubGD iter. 319/499: loss=4.4246315753453125, w0=73.24029999999998, w1=13.52362597849972\n",
      "SubGD iter. 320/499: loss=4.4246315736795605, w0=73.24029999999998, w1=13.523591831351164\n",
      "SubGD iter. 321/499: loss=4.424631572013805, w0=73.24029999999998, w1=13.523557684202608\n",
      "SubGD iter. 322/499: loss=4.424631570348052, w0=73.24029999999998, w1=13.523523537054052\n",
      "SubGD iter. 323/499: loss=4.4246315686822975, w0=73.24029999999998, w1=13.523489389905496\n",
      "SubGD iter. 324/499: loss=4.424631567016544, w0=73.24029999999998, w1=13.52345524275694\n",
      "SubGD iter. 325/499: loss=4.42463156535079, w0=73.24029999999998, w1=13.523421095608384\n",
      "SubGD iter. 326/499: loss=4.424631563685035, w0=73.24029999999998, w1=13.523386948459828\n",
      "SubGD iter. 327/499: loss=4.424631568278702, w0=73.24043999999998, w1=13.523609858085692\n",
      "SubGD iter. 328/499: loss=4.42463157289318, w0=73.24043999999998, w1=13.523575710937136\n",
      "SubGD iter. 329/499: loss=4.424631571227425, w0=73.24043999999998, w1=13.52354156378858\n",
      "SubGD iter. 330/499: loss=4.424631569561671, w0=73.24043999999998, w1=13.523507416640024\n",
      "SubGD iter. 331/499: loss=4.424631567895917, w0=73.24043999999998, w1=13.523473269491468\n",
      "SubGD iter. 332/499: loss=4.424631566230163, w0=73.24043999999998, w1=13.523439122342912\n",
      "SubGD iter. 333/499: loss=4.424631564564409, w0=73.24043999999998, w1=13.523404975194357\n",
      "SubGD iter. 334/499: loss=4.4246315628986554, w0=73.24043999999998, w1=13.5233708280458\n",
      "SubGD iter. 335/499: loss=4.424631561232901, w0=73.24043999999998, w1=13.523336680897245\n",
      "SubGD iter. 336/499: loss=4.424631559567148, w0=73.24043999999998, w1=13.523302533748689\n",
      "SubGD iter. 337/499: loss=4.424631567159919, w0=73.24057999999998, w1=13.523525443374552\n",
      "SubGD iter. 338/499: loss=4.424631568775291, w0=73.24057999999998, w1=13.523491296225997\n",
      "SubGD iter. 339/499: loss=4.424631567109538, w0=73.24057999999998, w1=13.52345714907744\n",
      "SubGD iter. 340/499: loss=4.424631565443783, w0=73.24057999999998, w1=13.523423001928885\n",
      "SubGD iter. 341/499: loss=4.424631563778029, w0=73.24057999999998, w1=13.523388854780329\n",
      "SubGD iter. 342/499: loss=4.424631562112276, w0=73.24057999999998, w1=13.523354707631773\n",
      "SubGD iter. 343/499: loss=4.424631560446521, w0=73.24057999999998, w1=13.523320560483217\n",
      "SubGD iter. 344/499: loss=4.424631558780767, w0=73.24057999999998, w1=13.523286413334661\n",
      "SubGD iter. 345/499: loss=4.424631557115013, w0=73.24057999999998, w1=13.523252266186105\n",
      "SubGD iter. 346/499: loss=4.424631555449259, w0=73.24057999999998, w1=13.52321811903755\n",
      "SubGD iter. 347/499: loss=4.4246315660411355, w0=73.24071999999998, w1=13.523441028663413\n",
      "SubGD iter. 348/499: loss=4.424631564657403, w0=73.24071999999998, w1=13.523406881514857\n",
      "SubGD iter. 349/499: loss=4.4246315629916495, w0=73.24071999999998, w1=13.523372734366301\n",
      "SubGD iter. 350/499: loss=4.424631561325894, w0=73.24071999999998, w1=13.523338587217745\n",
      "SubGD iter. 351/499: loss=4.424631559660142, w0=73.24071999999998, w1=13.52330444006919\n",
      "SubGD iter. 352/499: loss=4.4246315579943865, w0=73.24071999999998, w1=13.523270292920634\n",
      "SubGD iter. 353/499: loss=4.424631556328634, w0=73.24071999999998, w1=13.523236145772078\n",
      "SubGD iter. 354/499: loss=4.424631554662879, w0=73.24071999999998, w1=13.523201998623522\n",
      "SubGD iter. 355/499: loss=4.424631552997125, w0=73.24071999999998, w1=13.523167851474966\n",
      "SubGD iter. 356/499: loss=4.424631554048455, w0=73.24085999999998, w1=13.52339076110083\n",
      "SubGD iter. 357/499: loss=4.42463156220527, w0=73.24085999999998, w1=13.523356613952274\n",
      "SubGD iter. 358/499: loss=4.424631560539515, w0=73.24085999999998, w1=13.523322466803718\n",
      "SubGD iter. 359/499: loss=4.424631558873761, w0=73.24085999999998, w1=13.523288319655162\n",
      "SubGD iter. 360/499: loss=4.424631557208007, w0=73.24085999999998, w1=13.523254172506606\n",
      "SubGD iter. 361/499: loss=4.424631555542253, w0=73.24085999999998, w1=13.52322002535805\n",
      "SubGD iter. 362/499: loss=4.424631553876499, w0=73.24085999999998, w1=13.523185878209494\n",
      "SubGD iter. 363/499: loss=4.4246315522107444, w0=73.24085999999998, w1=13.523151731060938\n",
      "SubGD iter. 364/499: loss=4.424631550544991, w0=73.24085999999998, w1=13.523117583912382\n",
      "SubGD iter. 365/499: loss=4.424631548879238, w0=73.24085999999998, w1=13.523083436763827\n",
      "SubGD iter. 366/499: loss=4.424631552929672, w0=73.24099999999999, w1=13.52330634638969\n",
      "SubGD iter. 367/499: loss=4.4246315580873805, w0=73.24099999999999, w1=13.523272199241134\n",
      "SubGD iter. 368/499: loss=4.424631556421627, w0=73.24099999999999, w1=13.523238052092578\n",
      "SubGD iter. 369/499: loss=4.424631554755873, w0=73.24099999999999, w1=13.523203904944022\n",
      "SubGD iter. 370/499: loss=4.424631553090119, w0=73.24099999999999, w1=13.523169757795467\n",
      "SubGD iter. 371/499: loss=4.4246315514243655, w0=73.24099999999999, w1=13.52313561064691\n",
      "SubGD iter. 372/499: loss=4.424631549758611, w0=73.24099999999999, w1=13.523101463498355\n",
      "SubGD iter. 373/499: loss=4.424631548092857, w0=73.24099999999999, w1=13.523067316349799\n",
      "SubGD iter. 374/499: loss=4.424631546427102, w0=73.24099999999999, w1=13.523033169201243\n",
      "SubGD iter. 375/499: loss=4.424631544761348, w0=73.24099999999999, w1=13.522999022052687\n",
      "SubGD iter. 376/499: loss=4.424631551810888, w0=73.24113999999999, w1=13.52322193167855\n",
      "SubGD iter. 377/499: loss=4.424631553969492, w0=73.24113999999999, w1=13.523187784529995\n",
      "SubGD iter. 378/499: loss=4.4246315523037385, w0=73.24113999999999, w1=13.523153637381439\n",
      "SubGD iter. 379/499: loss=4.424631550637985, w0=73.24113999999999, w1=13.523119490232883\n",
      "SubGD iter. 380/499: loss=4.424631548972231, w0=73.24113999999999, w1=13.523085343084327\n",
      "SubGD iter. 381/499: loss=4.4246315473064755, w0=73.24113999999999, w1=13.523051195935771\n",
      "SubGD iter. 382/499: loss=4.4246315456407235, w0=73.24113999999999, w1=13.523017048787215\n",
      "SubGD iter. 383/499: loss=4.424631543974968, w0=73.24113999999999, w1=13.52298290163866\n",
      "SubGD iter. 384/499: loss=4.424631542309214, w0=73.24113999999999, w1=13.522948754490104\n",
      "SubGD iter. 385/499: loss=4.424631540643461, w0=73.24113999999999, w1=13.522914607341548\n",
      "SubGD iter. 386/499: loss=4.424631550692105, w0=73.24127999999999, w1=13.523137516967411\n",
      "SubGD iter. 387/499: loss=4.424631549851604, w0=73.24127999999999, w1=13.523103369818855\n",
      "SubGD iter. 388/499: loss=4.42463154818585, w0=73.24127999999999, w1=13.5230692226703\n",
      "SubGD iter. 389/499: loss=4.4246315465200965, w0=73.24127999999999, w1=13.523035075521744\n",
      "SubGD iter. 390/499: loss=4.424631544854342, w0=73.24127999999999, w1=13.523000928373188\n",
      "SubGD iter. 391/499: loss=4.424631543188589, w0=73.24127999999999, w1=13.522966781224632\n",
      "SubGD iter. 392/499: loss=4.424631541522834, w0=73.24127999999999, w1=13.522932634076076\n",
      "SubGD iter. 393/499: loss=4.424631539857081, w0=73.24127999999999, w1=13.52289848692752\n",
      "SubGD iter. 394/499: loss=4.424631538191327, w0=73.24127999999999, w1=13.522864339778964\n",
      "SubGD iter. 395/499: loss=4.4246315386994235, w0=73.24141999999999, w1=13.523087249404828\n",
      "SubGD iter. 396/499: loss=4.4246315473994695, w0=73.24141999999999, w1=13.523053102256272\n",
      "SubGD iter. 397/499: loss=4.424631545733717, w0=73.24141999999999, w1=13.523018955107716\n",
      "SubGD iter. 398/499: loss=4.424631544067962, w0=73.24141999999999, w1=13.52298480795916\n",
      "SubGD iter. 399/499: loss=4.424631542402208, w0=73.24141999999999, w1=13.522950660810604\n",
      "SubGD iter. 400/499: loss=4.4246315407364545, w0=73.24141999999999, w1=13.522916513662048\n",
      "SubGD iter. 401/499: loss=4.424631539070701, w0=73.24141999999999, w1=13.522882366513493\n",
      "SubGD iter. 402/499: loss=4.424631537404947, w0=73.24141999999999, w1=13.522848219364937\n",
      "SubGD iter. 403/499: loss=4.424631535739192, w0=73.24141999999999, w1=13.52281407221638\n",
      "SubGD iter. 404/499: loss=4.424631534073439, w0=73.24141999999999, w1=13.522779925067825\n",
      "SubGD iter. 405/499: loss=4.424631537580641, w0=73.24155999999999, w1=13.523002834693688\n",
      "SubGD iter. 406/499: loss=4.424631543281581, w0=73.24155999999999, w1=13.522968687545132\n",
      "SubGD iter. 407/499: loss=4.4246315416158275, w0=73.24155999999999, w1=13.522934540396577\n",
      "SubGD iter. 408/499: loss=4.424631539950075, w0=73.24155999999999, w1=13.52290039324802\n",
      "SubGD iter. 409/499: loss=4.42463153828432, w0=73.24155999999999, w1=13.522866246099465\n",
      "SubGD iter. 410/499: loss=4.424631536618566, w0=73.24155999999999, w1=13.522832098950909\n",
      "SubGD iter. 411/499: loss=4.4246315349528125, w0=73.24155999999999, w1=13.522797951802353\n",
      "SubGD iter. 412/499: loss=4.424631533287058, w0=73.24155999999999, w1=13.522763804653797\n",
      "SubGD iter. 413/499: loss=4.424631531621305, w0=73.24155999999999, w1=13.522729657505241\n",
      "SubGD iter. 414/499: loss=4.42463152995555, w0=73.24155999999999, w1=13.522695510356685\n",
      "SubGD iter. 415/499: loss=4.424631536461858, w0=73.2417, w1=13.522918419982549\n",
      "SubGD iter. 416/499: loss=4.4246315403358025, w0=73.24155999999999, w1=13.522905630681082\n",
      "SubGD iter. 417/499: loss=4.424631538539811, w0=73.24155999999999, w1=13.522871483532526\n",
      "SubGD iter. 418/499: loss=4.424631536874056, w0=73.24155999999999, w1=13.52283733638397\n",
      "SubGD iter. 419/499: loss=4.424631535208303, w0=73.24155999999999, w1=13.522803189235415\n",
      "SubGD iter. 420/499: loss=4.424631533542549, w0=73.24155999999999, w1=13.522769042086859\n",
      "SubGD iter. 421/499: loss=4.424631531876795, w0=73.24155999999999, w1=13.522734894938303\n",
      "SubGD iter. 422/499: loss=4.424631530211041, w0=73.24155999999999, w1=13.522700747789747\n",
      "SubGD iter. 423/499: loss=4.424631534794037, w0=73.2417, w1=13.52292365741561\n",
      "SubGD iter. 424/499: loss=4.424631540431494, w0=73.24155999999999, w1=13.522910868114144\n",
      "SubGD iter. 425/499: loss=4.4246315387953015, w0=73.24155999999999, w1=13.522876720965588\n",
      "SubGD iter. 426/499: loss=4.424631537129547, w0=73.24155999999999, w1=13.522842573817032\n",
      "SubGD iter. 427/499: loss=4.424631535463794, w0=73.24155999999999, w1=13.522808426668476\n",
      "SubGD iter. 428/499: loss=4.424631533798039, w0=73.24155999999999, w1=13.52277427951992\n",
      "SubGD iter. 429/499: loss=4.424631532132286, w0=73.24155999999999, w1=13.522740132371364\n",
      "SubGD iter. 430/499: loss=4.424631530466532, w0=73.24155999999999, w1=13.522705985222808\n",
      "SubGD iter. 431/499: loss=4.424631533126217, w0=73.2417, w1=13.522928894848672\n",
      "SubGD iter. 432/499: loss=4.424631540527184, w0=73.24155999999999, w1=13.522916105547205\n",
      "SubGD iter. 433/499: loss=4.424631539050792, w0=73.24155999999999, w1=13.52288195839865\n",
      "SubGD iter. 434/499: loss=4.424631537385038, w0=73.24155999999999, w1=13.522847811250093\n",
      "SubGD iter. 435/499: loss=4.424631535719284, w0=73.24155999999999, w1=13.522813664101538\n",
      "SubGD iter. 436/499: loss=4.42463153405353, w0=73.24155999999999, w1=13.522779516952982\n",
      "SubGD iter. 437/499: loss=4.424631532387776, w0=73.24155999999999, w1=13.522745369804426\n",
      "SubGD iter. 438/499: loss=4.424631530722022, w0=73.24155999999999, w1=13.52271122265587\n",
      "SubGD iter. 439/499: loss=4.424631531458396, w0=73.2417, w1=13.522934132281733\n",
      "SubGD iter. 440/499: loss=4.424631540622874, w0=73.24155999999999, w1=13.522921342980267\n",
      "SubGD iter. 441/499: loss=4.424631539306283, w0=73.24155999999999, w1=13.52288719583171\n",
      "SubGD iter. 442/499: loss=4.424631537640528, w0=73.24155999999999, w1=13.522853048683155\n",
      "SubGD iter. 443/499: loss=4.4246315359747745, w0=73.24155999999999, w1=13.522818901534599\n",
      "SubGD iter. 444/499: loss=4.424631534309021, w0=73.24155999999999, w1=13.522784754386043\n",
      "SubGD iter. 445/499: loss=4.424631532643267, w0=73.24155999999999, w1=13.522750607237487\n",
      "SubGD iter. 446/499: loss=4.424631530977512, w0=73.24155999999999, w1=13.522716460088931\n",
      "SubGD iter. 447/499: loss=4.4246315297905765, w0=73.2417, w1=13.522939369714795\n",
      "SubGD iter. 448/499: loss=4.424631540718565, w0=73.24155999999999, w1=13.522926580413328\n",
      "SubGD iter. 449/499: loss=4.424631539561774, w0=73.24155999999999, w1=13.522892433264772\n",
      "SubGD iter. 450/499: loss=4.424631537896019, w0=73.24155999999999, w1=13.522858286116216\n",
      "SubGD iter. 451/499: loss=4.424631536230265, w0=73.24155999999999, w1=13.52282413896766\n",
      "SubGD iter. 452/499: loss=4.4246315345645115, w0=73.24155999999999, w1=13.522789991819105\n",
      "SubGD iter. 453/499: loss=4.424631532898758, w0=73.24155999999999, w1=13.522755844670549\n",
      "SubGD iter. 454/499: loss=4.424631531233003, w0=73.24155999999999, w1=13.522721697521993\n",
      "SubGD iter. 455/499: loss=4.424631529567249, w0=73.24155999999999, w1=13.522687550373437\n",
      "SubGD iter. 456/499: loss=4.424631538996652, w0=73.2417, w1=13.5229104599993\n",
      "SubGD iter. 457/499: loss=4.424631540190371, w0=73.24155999999999, w1=13.522897670697834\n",
      "SubGD iter. 458/499: loss=4.42463153815151, w0=73.24155999999999, w1=13.522863523549278\n",
      "SubGD iter. 459/499: loss=4.424631536485755, w0=73.24155999999999, w1=13.522829376400722\n",
      "SubGD iter. 460/499: loss=4.424631534820002, w0=73.24155999999999, w1=13.522795229252166\n",
      "SubGD iter. 461/499: loss=4.4246315331542485, w0=73.24155999999999, w1=13.52276108210361\n",
      "SubGD iter. 462/499: loss=4.424631531488494, w0=73.24155999999999, w1=13.522726934955054\n",
      "SubGD iter. 463/499: loss=4.42463152982274, w0=73.24155999999999, w1=13.522692787806498\n",
      "SubGD iter. 464/499: loss=4.424631537328833, w0=73.2417, w1=13.522915697432362\n",
      "SubGD iter. 465/499: loss=4.424631540286062, w0=73.24155999999999, w1=13.522902908130895\n",
      "SubGD iter. 466/499: loss=4.4246315384070005, w0=73.24155999999999, w1=13.52286876098234\n",
      "SubGD iter. 467/499: loss=4.424631536741246, w0=73.24155999999999, w1=13.522834613833783\n",
      "SubGD iter. 468/499: loss=4.424631535075493, w0=73.24155999999999, w1=13.522800466685228\n",
      "SubGD iter. 469/499: loss=4.424631533409738, w0=73.24155999999999, w1=13.522766319536672\n",
      "SubGD iter. 470/499: loss=4.424631531743984, w0=73.24155999999999, w1=13.522732172388116\n",
      "SubGD iter. 471/499: loss=4.424631530078232, w0=73.24155999999999, w1=13.52269802523956\n",
      "SubGD iter. 472/499: loss=4.424631535661012, w0=73.2417, w1=13.522920934865423\n",
      "SubGD iter. 473/499: loss=4.4246315403817515, w0=73.24155999999999, w1=13.522908145563957\n",
      "SubGD iter. 474/499: loss=4.424631538662491, w0=73.24155999999999, w1=13.5228739984154\n",
      "SubGD iter. 475/499: loss=4.4246315369967375, w0=73.24155999999999, w1=13.522839851266845\n",
      "SubGD iter. 476/499: loss=4.424631535330982, w0=73.24155999999999, w1=13.522805704118289\n",
      "SubGD iter. 477/499: loss=4.42463153366523, w0=73.24155999999999, w1=13.522771556969733\n",
      "SubGD iter. 478/499: loss=4.4246315319994745, w0=73.24155999999999, w1=13.522737409821177\n",
      "SubGD iter. 479/499: loss=4.424631530333721, w0=73.24155999999999, w1=13.522703262672621\n",
      "SubGD iter. 480/499: loss=4.424631533993192, w0=73.2417, w1=13.522926172298485\n",
      "SubGD iter. 481/499: loss=4.424631540477442, w0=73.24155999999999, w1=13.522913382997018\n",
      "SubGD iter. 482/499: loss=4.424631538917981, w0=73.24155999999999, w1=13.522879235848462\n",
      "SubGD iter. 483/499: loss=4.424631537252228, w0=73.24155999999999, w1=13.522845088699906\n",
      "SubGD iter. 484/499: loss=4.424631535586474, w0=73.24155999999999, w1=13.52281094155135\n",
      "SubGD iter. 485/499: loss=4.42463153392072, w0=73.24155999999999, w1=13.522776794402795\n",
      "SubGD iter. 486/499: loss=4.424631532254966, w0=73.24155999999999, w1=13.522742647254239\n",
      "SubGD iter. 487/499: loss=4.424631530589211, w0=73.24155999999999, w1=13.522708500105683\n",
      "SubGD iter. 488/499: loss=4.424631532325371, w0=73.2417, w1=13.522931409731546\n",
      "SubGD iter. 489/499: loss=4.424631540573133, w0=73.24155999999999, w1=13.52291862043008\n",
      "SubGD iter. 490/499: loss=4.424631539173472, w0=73.24155999999999, w1=13.522884473281524\n",
      "SubGD iter. 491/499: loss=4.424631537507717, w0=73.24155999999999, w1=13.522850326132968\n",
      "SubGD iter. 492/499: loss=4.424631535841964, w0=73.24155999999999, w1=13.522816178984412\n",
      "SubGD iter. 493/499: loss=4.424631534176211, w0=73.24155999999999, w1=13.522782031835856\n",
      "SubGD iter. 494/499: loss=4.424631532510456, w0=73.24155999999999, w1=13.5227478846873\n",
      "SubGD iter. 495/499: loss=4.424631530844702, w0=73.24155999999999, w1=13.522713737538744\n",
      "SubGD iter. 496/499: loss=4.424631530657551, w0=73.2417, w1=13.522936647164608\n",
      "SubGD iter. 497/499: loss=4.424631540668822, w0=73.24155999999999, w1=13.522923857863141\n",
      "SubGD iter. 498/499: loss=4.424631539428963, w0=73.24155999999999, w1=13.522889710714585\n",
      "SubGD iter. 499/499: loss=4.424631537763208, w0=73.24155999999999, w1=13.52285556356603\n",
      "SubGD: execution time=1.601 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49e4aed6d4f41129278806375b953fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        index = np.random.randint(0,len(y),batch_size)# select randomly batch_size element from y to compute loss and grid\n",
    "        ys,txs = y[index],tx[index] #batch from y\n",
    "        loss = np.mean(np.abs(ys - txs.dot(w)))\n",
    "        grid = compute_subgradient_mae(ys,txs,w)\n",
    "\n",
    "        w = w - gamma*grid\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=54.68080451840291, w0=0.7, w1=-0.973234543164873\n",
      "SubSGD iter. 1/499: loss=64.74362394929184, w0=1.4, w1=-1.1345715060118011\n",
      "SubSGD iter. 2/499: loss=63.66821090937238, w0=2.0999999999999996, w1=-1.0220638998251608\n",
      "SubSGD iter. 3/499: loss=97.33633835427287, w0=2.8, w1=-0.18823569978986543\n",
      "SubSGD iter. 4/499: loss=56.63704134326494, w0=3.5, w1=-1.1179676527046323\n",
      "SubSGD iter. 5/499: loss=74.00328415863316, w0=4.2, w1=-0.536546308471064\n",
      "SubSGD iter. 6/499: loss=84.16349225263784, w0=4.9, w1=-0.12433419066136314\n",
      "SubSGD iter. 7/499: loss=68.32175335738535, w0=5.6000000000000005, w1=0.17391660618920896\n",
      "SubSGD iter. 8/499: loss=96.5620809187256, w0=6.300000000000001, w1=1.283266140682362\n",
      "SubSGD iter. 9/499: loss=69.13503132631334, w0=7.000000000000001, w1=1.257556227906302\n",
      "SubSGD iter. 10/499: loss=82.19632043861243, w0=7.700000000000001, w1=2.254698127095801\n",
      "SubSGD iter. 11/499: loss=67.37192026566956, w0=8.4, w1=2.873873659458016\n",
      "SubSGD iter. 12/499: loss=85.29605309288328, w0=9.1, w1=3.6874469404564394\n",
      "SubSGD iter. 13/499: loss=65.64660133705304, w0=9.799999999999999, w1=3.7606757460236\n",
      "SubSGD iter. 14/499: loss=51.52598780048209, w0=10.499999999999998, w1=3.426160289431266\n",
      "SubSGD iter. 15/499: loss=71.29014916598413, w0=11.199999999999998, w1=4.195686407853239\n",
      "SubSGD iter. 16/499: loss=54.529458668158746, w0=11.899999999999997, w1=3.5818052319061815\n",
      "SubSGD iter. 17/499: loss=47.08169912415533, w0=12.599999999999996, w1=1.6481356595160688\n",
      "SubSGD iter. 18/499: loss=60.69144083679379, w0=13.299999999999995, w1=1.9710838058712472\n",
      "SubSGD iter. 19/499: loss=44.37521394117323, w0=13.999999999999995, w1=0.936079937243707\n",
      "SubSGD iter. 20/499: loss=70.69756213399923, w0=14.699999999999994, w1=1.0821445793765725\n",
      "SubSGD iter. 21/499: loss=67.7500016747648, w0=15.399999999999993, w1=1.1457998957092492\n",
      "SubSGD iter. 22/499: loss=58.434121042905, w0=16.099999999999994, w1=0.7986968764670004\n",
      "SubSGD iter. 23/499: loss=73.61603496661893, w0=16.799999999999994, w1=1.7381868453531961\n",
      "SubSGD iter. 24/499: loss=53.44363464399877, w0=17.499999999999993, w1=1.0069095606702887\n",
      "SubSGD iter. 25/499: loss=54.1163518350488, w0=18.199999999999992, w1=0.45777620662983465\n",
      "SubSGD iter. 26/499: loss=69.57538183092981, w0=18.89999999999999, w1=1.8134370366523997\n",
      "SubSGD iter. 27/499: loss=48.487669786571544, w0=19.59999999999999, w1=1.834239711573461\n",
      "SubSGD iter. 28/499: loss=55.803226899929925, w0=20.29999999999999, w1=1.740036123817099\n",
      "SubSGD iter. 29/499: loss=35.75111135498992, w0=20.99999999999999, w1=0.5426990303234802\n",
      "SubSGD iter. 30/499: loss=48.2083750796425, w0=21.69999999999999, w1=0.2504360486730328\n",
      "SubSGD iter. 31/499: loss=67.14390666560043, w0=22.399999999999988, w1=0.4647680122449224\n",
      "SubSGD iter. 32/499: loss=65.94367130504693, w0=23.099999999999987, w1=0.8292535341637362\n",
      "SubSGD iter. 33/499: loss=49.29737980512676, w0=23.799999999999986, w1=0.9427152639474674\n",
      "SubSGD iter. 34/499: loss=50.17491197689233, w0=24.499999999999986, w1=0.2937301045684032\n",
      "SubSGD iter. 35/499: loss=53.43540201445235, w0=25.199999999999985, w1=-0.0025583688884981415\n",
      "SubSGD iter. 36/499: loss=62.21153197061544, w0=25.899999999999984, w1=0.37927882414273484\n",
      "SubSGD iter. 37/499: loss=29.571787616092113, w0=26.599999999999984, w1=-0.4126412291216822\n",
      "SubSGD iter. 38/499: loss=60.27134030980595, w0=27.299999999999983, w1=0.30879537908961463\n",
      "SubSGD iter. 39/499: loss=23.493687484726337, w0=27.999999999999982, w1=-0.6782085524789698\n",
      "SubSGD iter. 40/499: loss=36.33795636083557, w0=28.69999999999998, w1=-1.1104864016422225\n",
      "SubSGD iter. 41/499: loss=67.14835509807148, w0=29.39999999999998, w1=-0.1593410596588426\n",
      "SubSGD iter. 42/499: loss=57.924573167545304, w0=30.09999999999998, w1=0.5551998067448659\n",
      "SubSGD iter. 43/499: loss=27.591248232788054, w0=30.79999999999998, w1=0.0028818278384626916\n",
      "SubSGD iter. 44/499: loss=52.096982689001024, w0=31.49999999999998, w1=-0.28701375738603513\n",
      "SubSGD iter. 45/499: loss=26.846382650398542, w0=32.19999999999998, w1=-1.3136415060596147\n",
      "SubSGD iter. 46/499: loss=44.11839240739328, w0=32.899999999999984, w1=-1.352708887415798\n",
      "SubSGD iter. 47/499: loss=31.663503776873142, w0=33.59999999999999, w1=-1.6428758953457079\n",
      "SubSGD iter. 48/499: loss=35.98043573772328, w0=34.29999999999999, w1=-1.6651765648129293\n",
      "SubSGD iter. 49/499: loss=33.631779296100056, w0=34.99999999999999, w1=-2.0521948607304266\n",
      "SubSGD iter. 50/499: loss=16.95254977525314, w0=35.699999999999996, w1=-2.5595600422744176\n",
      "SubSGD iter. 51/499: loss=11.116491452820682, w0=36.4, w1=-3.5008710317222684\n",
      "SubSGD iter. 52/499: loss=26.02609913599332, w0=37.1, w1=-4.2630764049054966\n",
      "SubSGD iter. 53/499: loss=28.678296754165707, w0=37.800000000000004, w1=-4.803124690607249\n",
      "SubSGD iter. 54/499: loss=62.18107367916781, w0=38.50000000000001, w1=-3.808689431165274\n",
      "SubSGD iter. 55/499: loss=11.343424374244812, w0=39.20000000000001, w1=-4.402411622146252\n",
      "SubSGD iter. 56/499: loss=8.81662193677051, w0=39.90000000000001, w1=-5.427794461797425\n",
      "SubSGD iter. 57/499: loss=50.68251565834747, w0=40.600000000000016, w1=-5.213462498225535\n",
      "SubSGD iter. 58/499: loss=25.162341956677075, w0=41.30000000000002, w1=-5.683337030833083\n",
      "SubSGD iter. 59/499: loss=71.17054073624796, w0=42.00000000000002, w1=-4.050566330823409\n",
      "SubSGD iter. 60/499: loss=9.760065590294623, w0=42.700000000000024, w1=-4.660861915152139\n",
      "SubSGD iter. 61/499: loss=36.541462004584645, w0=43.40000000000003, w1=-4.3469346557133655\n",
      "SubSGD iter. 62/499: loss=55.45159643717216, w0=44.10000000000003, w1=-3.639904666235642\n",
      "SubSGD iter. 63/499: loss=23.234116149356687, w0=44.80000000000003, w1=-4.149129706243416\n",
      "SubSGD iter. 64/499: loss=44.8661040039756, w0=45.500000000000036, w1=-3.5604416616644334\n",
      "SubSGD iter. 65/499: loss=1.4433746951587167, w0=44.80000000000003, w1=-2.3217823007147493\n",
      "SubSGD iter. 66/499: loss=39.8060490090684, w0=45.500000000000036, w1=-1.726034319436864\n",
      "SubSGD iter. 67/499: loss=52.35396440304118, w0=46.20000000000004, w1=-0.6944644487174598\n",
      "SubSGD iter. 68/499: loss=37.25926308375874, w0=46.90000000000004, w1=-0.4898394786585458\n",
      "SubSGD iter. 69/499: loss=50.02504534102065, w0=47.600000000000044, w1=0.8277171579945435\n",
      "SubSGD iter. 70/499: loss=28.58057280498774, w0=48.30000000000005, w1=1.2678410690648678\n",
      "SubSGD iter. 71/499: loss=7.216554852868924, w0=49.00000000000005, w1=0.5806336851475226\n",
      "SubSGD iter. 72/499: loss=19.93406491096922, w0=49.70000000000005, w1=0.04411760970266598\n",
      "SubSGD iter. 73/499: loss=31.963977910631392, w0=50.400000000000055, w1=0.48419608144795556\n",
      "SubSGD iter. 74/499: loss=32.21848141964365, w0=51.10000000000006, w1=1.0667220386879208\n",
      "SubSGD iter. 75/499: loss=8.040344460300744, w0=51.80000000000006, w1=0.5582943669365531\n",
      "SubSGD iter. 76/499: loss=5.957141965599732, w0=51.10000000000006, w1=1.8191906101942088\n",
      "SubSGD iter. 77/499: loss=24.138766370845296, w0=51.80000000000006, w1=1.782584101253113\n",
      "SubSGD iter. 78/499: loss=4.696423613021608, w0=52.500000000000064, w1=1.2690806890051085\n",
      "SubSGD iter. 79/499: loss=38.385311518684375, w0=53.20000000000007, w1=1.7976742756321622\n",
      "SubSGD iter. 80/499: loss=1.9880281612706625, w0=53.90000000000007, w1=0.7117322795329886\n",
      "SubSGD iter. 81/499: loss=13.310526345871118, w0=54.60000000000007, w1=-0.0050265082198589495\n",
      "SubSGD iter. 82/499: loss=20.625191004756353, w0=55.300000000000075, w1=-0.4777981781746616\n",
      "SubSGD iter. 83/499: loss=24.27691561029092, w0=56.00000000000008, w1=-0.35076552712992803\n",
      "SubSGD iter. 84/499: loss=35.25128269701864, w0=56.70000000000008, w1=0.9909966012643115\n",
      "SubSGD iter. 85/499: loss=5.348817441502668, w0=57.400000000000084, w1=0.3696141938552454\n",
      "SubSGD iter. 86/499: loss=6.727900665273488, w0=58.10000000000009, w1=-0.07743043727301091\n",
      "SubSGD iter. 87/499: loss=7.58780030131868, w0=57.400000000000084, w1=0.6663905334922121\n",
      "SubSGD iter. 88/499: loss=33.81637159404475, w0=58.10000000000009, w1=1.3159019033206065\n",
      "SubSGD iter. 89/499: loss=10.808492797259348, w0=57.400000000000084, w1=2.7585931146976437\n",
      "SubSGD iter. 90/499: loss=13.783763239484259, w0=58.10000000000009, w1=2.7565144335705143\n",
      "SubSGD iter. 91/499: loss=29.976574235457264, w0=58.80000000000009, w1=2.9708463971424037\n",
      "SubSGD iter. 92/499: loss=14.270083662262799, w0=59.50000000000009, w1=2.5249048143440374\n",
      "SubSGD iter. 93/499: loss=13.02253047826703, w0=60.200000000000095, w1=2.5722449328244252\n",
      "SubSGD iter. 94/499: loss=7.495047685303561, w0=60.9000000000001, w1=1.8684442665721455\n",
      "SubSGD iter. 95/499: loss=10.656993173010271, w0=61.6000000000001, w1=2.013933051224048\n",
      "SubSGD iter. 96/499: loss=0.6863593832342048, w0=62.300000000000104, w1=2.0844552090095783\n",
      "SubSGD iter. 97/499: loss=21.714350399326044, w0=61.6000000000001, w1=3.1817564466706276\n",
      "SubSGD iter. 98/499: loss=21.73631441309331, w0=62.300000000000104, w1=4.168326779744664\n",
      "SubSGD iter. 99/499: loss=18.759623883044995, w0=63.00000000000011, w1=4.524358561696667\n",
      "SubSGD iter. 100/499: loss=1.3982836047505245, w0=63.70000000000011, w1=3.843430869326951\n",
      "SubSGD iter. 101/499: loss=4.4156964512961565, w0=64.4000000000001, w1=4.311044523076509\n",
      "SubSGD iter. 102/499: loss=17.798973275445533, w0=65.10000000000011, w1=4.73544087611109\n",
      "SubSGD iter. 103/499: loss=11.430187293329453, w0=65.80000000000011, w1=5.196920617546553\n",
      "SubSGD iter. 104/499: loss=15.15621798375743, w0=66.50000000000011, w1=5.584803755190632\n",
      "SubSGD iter. 105/499: loss=3.682275592845315, w0=67.20000000000012, w1=5.912613901945307\n",
      "SubSGD iter. 106/499: loss=1.8518123053528086, w0=66.50000000000011, w1=6.656924801671153\n",
      "SubSGD iter. 107/499: loss=3.3637370585157953, w0=67.20000000000012, w1=6.516100665082614\n",
      "SubSGD iter. 108/499: loss=14.495734352701675, w0=67.90000000000012, w1=6.422698766785963\n",
      "SubSGD iter. 109/499: loss=7.794720322452463, w0=68.60000000000012, w1=6.830900656433728\n",
      "SubSGD iter. 110/499: loss=0.7968769259482116, w0=67.90000000000012, w1=7.583138625353687\n",
      "SubSGD iter. 111/499: loss=22.46428456397618, w0=68.60000000000012, w1=8.938226537174046\n",
      "SubSGD iter. 112/499: loss=4.525353590475127, w0=67.90000000000012, w1=9.906894630432367\n",
      "SubSGD iter. 113/499: loss=6.011884803050819, w0=68.60000000000012, w1=10.845883432190226\n",
      "SubSGD iter. 114/499: loss=0.14191156810514372, w0=69.30000000000013, w1=10.552149518397133\n",
      "SubSGD iter. 115/499: loss=4.555172141457305, w0=70.00000000000013, w1=11.019949512901455\n",
      "SubSGD iter. 116/499: loss=2.854325743902109, w0=69.30000000000013, w1=11.230793490462265\n",
      "SubSGD iter. 117/499: loss=5.737013112482721, w0=70.00000000000013, w1=9.687073961959845\n",
      "SubSGD iter. 118/499: loss=5.599981518465171, w0=70.70000000000013, w1=9.840121047427955\n",
      "SubSGD iter. 119/499: loss=2.4536730879731294, w0=71.40000000000013, w1=10.772151374072374\n",
      "SubSGD iter. 120/499: loss=4.841242586966743, w0=72.10000000000014, w1=10.773101550526933\n",
      "SubSGD iter. 121/499: loss=3.675111652936934, w0=71.40000000000013, w1=10.727090085904544\n",
      "SubSGD iter. 122/499: loss=4.976843574002295, w0=70.70000000000013, w1=11.592266645327957\n",
      "SubSGD iter. 123/499: loss=7.960476356246879, w0=70.00000000000013, w1=11.949194982878018\n",
      "SubSGD iter. 124/499: loss=4.391530519115335, w0=70.70000000000013, w1=12.759550623345662\n",
      "SubSGD iter. 125/499: loss=1.5160850188962058, w0=71.40000000000013, w1=13.979780638869258\n",
      "SubSGD iter. 126/499: loss=4.572548772745122, w0=72.10000000000014, w1=12.935906604455846\n",
      "SubSGD iter. 127/499: loss=8.310994367252263, w0=71.40000000000013, w1=13.749023640161074\n",
      "SubSGD iter. 128/499: loss=2.0411873936266716, w0=72.10000000000014, w1=13.645679588251982\n",
      "SubSGD iter. 129/499: loss=5.056204384544884, w0=72.80000000000014, w1=13.289198988333647\n",
      "SubSGD iter. 130/499: loss=12.718750210136136, w0=73.50000000000014, w1=12.71848627253249\n",
      "SubSGD iter. 131/499: loss=0.8786446074898464, w0=74.20000000000014, w1=12.292712822173396\n",
      "SubSGD iter. 132/499: loss=3.0807304429936835, w0=73.50000000000014, w1=10.54015506261459\n",
      "SubSGD iter. 133/499: loss=2.013190673772101, w0=74.20000000000014, w1=10.997954913312856\n",
      "SubSGD iter. 134/499: loss=4.065441306392287, w0=73.50000000000014, w1=11.515513931099273\n",
      "SubSGD iter. 135/499: loss=1.5225933837770782, w0=74.20000000000014, w1=11.790795237146565\n",
      "SubSGD iter. 136/499: loss=1.6779568502815252, w0=74.90000000000015, w1=10.791804881796383\n",
      "SubSGD iter. 137/499: loss=2.5056298022185572, w0=74.20000000000014, w1=12.172839761698839\n",
      "SubSGD iter. 138/499: loss=3.5132452227178987, w0=73.50000000000014, w1=12.016043525918763\n",
      "SubSGD iter. 139/499: loss=0.43166917684555983, w0=74.20000000000014, w1=11.858013324099506\n",
      "SubSGD iter. 140/499: loss=7.485638948815911, w0=74.90000000000015, w1=11.255367505239631\n",
      "SubSGD iter. 141/499: loss=7.767947164288778, w0=74.20000000000014, w1=12.076501303777869\n",
      "SubSGD iter. 142/499: loss=5.351195488005956, w0=73.50000000000014, w1=11.814118957508885\n",
      "SubSGD iter. 143/499: loss=2.369373380783344, w0=74.20000000000014, w1=11.394430431386168\n",
      "SubSGD iter. 144/499: loss=0.7372567868987829, w0=73.50000000000014, w1=11.6028251775998\n",
      "SubSGD iter. 145/499: loss=1.8500382267184676, w0=74.20000000000014, w1=11.908352283160323\n",
      "SubSGD iter. 146/499: loss=3.3422149038630664, w0=74.90000000000015, w1=13.743989540534528\n",
      "SubSGD iter. 147/499: loss=6.191147199913829, w0=74.20000000000014, w1=14.210408185265429\n",
      "SubSGD iter. 148/499: loss=1.6474492750927894, w0=74.90000000000015, w1=13.349038652835464\n",
      "SubSGD iter. 149/499: loss=7.965107549680553, w0=75.60000000000015, w1=12.82605551216755\n",
      "SubSGD iter. 150/499: loss=2.0644919085394378, w0=76.30000000000015, w1=13.615062241693039\n",
      "SubSGD iter. 151/499: loss=5.976612490534656, w0=77.00000000000016, w1=12.90290093788757\n",
      "SubSGD iter. 152/499: loss=1.7443950696851829, w0=76.30000000000015, w1=13.50856831992309\n",
      "SubSGD iter. 153/499: loss=7.601910507211926, w0=75.60000000000015, w1=13.499151562396596\n",
      "SubSGD iter. 154/499: loss=2.9123679983159576, w0=76.30000000000015, w1=13.962878482636437\n",
      "SubSGD iter. 155/499: loss=2.920611984475343, w0=77.00000000000016, w1=13.636746587693764\n",
      "SubSGD iter. 156/499: loss=4.036591390825549, w0=76.30000000000015, w1=14.165135955629768\n",
      "SubSGD iter. 157/499: loss=0.2571971037157823, w0=77.00000000000016, w1=14.46025878640729\n",
      "SubSGD iter. 158/499: loss=10.736741645949486, w0=76.30000000000015, w1=14.928349911561645\n",
      "SubSGD iter. 159/499: loss=8.200007714990612, w0=75.60000000000015, w1=15.8630722534619\n",
      "SubSGD iter. 160/499: loss=6.085532233536185, w0=74.90000000000015, w1=15.192526723815382\n",
      "SubSGD iter. 161/499: loss=10.931823713487702, w0=75.60000000000015, w1=15.155951074225879\n",
      "SubSGD iter. 162/499: loss=2.1565053377266423, w0=74.90000000000015, w1=15.10794658125324\n",
      "SubSGD iter. 163/499: loss=4.2702100888922985, w0=74.20000000000014, w1=14.654450442528717\n",
      "SubSGD iter. 164/499: loss=2.039986182040181, w0=73.50000000000014, w1=15.004066216062766\n",
      "SubSGD iter. 165/499: loss=0.6671081331707853, w0=74.20000000000014, w1=16.152024500364845\n",
      "SubSGD iter. 166/499: loss=1.3114351942934945, w0=74.90000000000015, w1=15.554324707615015\n",
      "SubSGD iter. 167/499: loss=5.8358925056763695, w0=75.60000000000015, w1=15.655879366477423\n",
      "SubSGD iter. 168/499: loss=2.7019869024291125, w0=74.90000000000015, w1=14.48567606746586\n",
      "SubSGD iter. 169/499: loss=8.257332395861553, w0=74.20000000000014, w1=14.705847735361456\n",
      "SubSGD iter. 170/499: loss=3.750197542947717, w0=74.90000000000015, w1=14.23470708187178\n",
      "SubSGD iter. 171/499: loss=5.075659084602194, w0=74.20000000000014, w1=14.882882165763256\n",
      "SubSGD iter. 172/499: loss=10.433293387362028, w0=73.50000000000014, w1=13.227658019926327\n",
      "SubSGD iter. 173/499: loss=3.2361321076026712, w0=74.20000000000014, w1=13.870016417931703\n",
      "SubSGD iter. 174/499: loss=4.153010988348967, w0=73.50000000000014, w1=13.673975132273208\n",
      "SubSGD iter. 175/499: loss=3.748381827736054, w0=72.80000000000014, w1=13.683069968327024\n",
      "SubSGD iter. 176/499: loss=8.99849284039108, w0=73.50000000000014, w1=14.189882970483556\n",
      "SubSGD iter. 177/499: loss=9.790889129423853, w0=72.80000000000014, w1=14.685774211289768\n",
      "SubSGD iter. 178/499: loss=5.424281947970655, w0=72.10000000000014, w1=14.493194353721163\n",
      "SubSGD iter. 179/499: loss=0.8768773990454548, w0=71.40000000000013, w1=13.429475772691\n",
      "SubSGD iter. 180/499: loss=11.949398764147787, w0=72.10000000000014, w1=13.463868139684688\n",
      "SubSGD iter. 181/499: loss=1.5689419804274536, w0=71.40000000000013, w1=12.455412769873396\n",
      "SubSGD iter. 182/499: loss=0.06064133803387506, w0=70.70000000000013, w1=13.501114315832943\n",
      "SubSGD iter. 183/499: loss=7.626429366390354, w0=71.40000000000013, w1=13.797664303377701\n",
      "SubSGD iter. 184/499: loss=0.971673715675756, w0=70.70000000000013, w1=14.282162456929543\n",
      "SubSGD iter. 185/499: loss=8.104580759130336, w0=71.40000000000013, w1=13.320876132797565\n",
      "SubSGD iter. 186/499: loss=3.2254495698063153, w0=72.10000000000014, w1=14.817762287101425\n",
      "SubSGD iter. 187/499: loss=3.7785579702111463, w0=71.40000000000013, w1=15.419445501094348\n",
      "SubSGD iter. 188/499: loss=3.6008802872767376, w0=70.70000000000013, w1=14.313575850628846\n",
      "SubSGD iter. 189/499: loss=12.299196479843502, w0=71.40000000000013, w1=14.695296699088773\n",
      "SubSGD iter. 190/499: loss=5.73679345018202, w0=72.10000000000014, w1=15.665932093948923\n",
      "SubSGD iter. 191/499: loss=3.718449067790516, w0=71.40000000000013, w1=14.884522297187264\n",
      "SubSGD iter. 192/499: loss=11.920940445311771, w0=72.10000000000014, w1=14.593559584231967\n",
      "SubSGD iter. 193/499: loss=0.24785934291752199, w0=72.80000000000014, w1=15.258957273318384\n",
      "SubSGD iter. 194/499: loss=1.2716398294409572, w0=73.50000000000014, w1=14.258815333934827\n",
      "SubSGD iter. 195/499: loss=3.8017118617553365, w0=72.80000000000014, w1=12.681205101236133\n",
      "SubSGD iter. 196/499: loss=2.924897914929659, w0=73.50000000000014, w1=11.8982824023016\n",
      "SubSGD iter. 197/499: loss=8.797619806458087, w0=74.20000000000014, w1=12.104862946352727\n",
      "SubSGD iter. 198/499: loss=5.969685436492348, w0=73.50000000000014, w1=12.40957169521222\n",
      "SubSGD iter. 199/499: loss=16.08981535760786, w0=74.20000000000014, w1=11.362766381636956\n",
      "SubSGD iter. 200/499: loss=1.4032322425855313, w0=74.90000000000015, w1=10.256044457604876\n",
      "SubSGD iter. 201/499: loss=1.6561667413536298, w0=74.20000000000014, w1=9.552800312050802\n",
      "SubSGD iter. 202/499: loss=6.069835173546224, w0=73.50000000000014, w1=9.659592660872729\n",
      "SubSGD iter. 203/499: loss=3.9335356499537255, w0=74.20000000000014, w1=10.099446409634167\n",
      "SubSGD iter. 204/499: loss=8.81327987039586, w0=73.50000000000014, w1=10.553697126940516\n",
      "SubSGD iter. 205/499: loss=0.32361025162630597, w0=74.20000000000014, w1=10.9896176892387\n",
      "SubSGD iter. 206/499: loss=7.182232529410896, w0=73.50000000000014, w1=11.650243259090287\n",
      "SubSGD iter. 207/499: loss=1.7695880642692998, w0=74.20000000000014, w1=12.022208499837976\n",
      "SubSGD iter. 208/499: loss=3.222031423211135, w0=74.90000000000015, w1=11.442913321902683\n",
      "SubSGD iter. 209/499: loss=6.781201251105848, w0=74.20000000000014, w1=12.087943281500934\n",
      "SubSGD iter. 210/499: loss=2.286503696515318, w0=73.50000000000014, w1=12.78184681628435\n",
      "SubSGD iter. 211/499: loss=4.629992543658133, w0=72.80000000000014, w1=13.400281860450054\n",
      "SubSGD iter. 212/499: loss=3.518468166884496, w0=72.10000000000014, w1=13.942985531906618\n",
      "SubSGD iter. 213/499: loss=4.222531557353975, w0=72.80000000000014, w1=14.588438155423683\n",
      "SubSGD iter. 214/499: loss=6.839149716007313, w0=72.10000000000014, w1=13.201288328868925\n",
      "SubSGD iter. 215/499: loss=2.2649127417567456, w0=72.80000000000014, w1=13.90129772882326\n",
      "SubSGD iter. 216/499: loss=2.9385226036155956, w0=72.10000000000014, w1=13.756789382830284\n",
      "SubSGD iter. 217/499: loss=0.44193396510685545, w0=72.80000000000014, w1=13.719558363707613\n",
      "SubSGD iter. 218/499: loss=6.206736152034338, w0=73.50000000000014, w1=13.972383860510423\n",
      "SubSGD iter. 219/499: loss=9.68798648640967, w0=72.80000000000014, w1=12.51994530039607\n",
      "SubSGD iter. 220/499: loss=0.7042592728645047, w0=72.10000000000014, w1=12.260068434080402\n",
      "SubSGD iter. 221/499: loss=8.194264725820119, w0=72.80000000000014, w1=12.839005579425484\n",
      "SubSGD iter. 222/499: loss=4.209800649844581, w0=73.50000000000014, w1=12.041161671613974\n",
      "SubSGD iter. 223/499: loss=1.9789506810768671, w0=72.80000000000014, w1=10.730122716604379\n",
      "SubSGD iter. 224/499: loss=2.081012198158831, w0=73.50000000000014, w1=11.217060934601887\n",
      "SubSGD iter. 225/499: loss=1.7385672779038543, w0=72.80000000000014, w1=11.46573364267135\n",
      "SubSGD iter. 226/499: loss=0.4390098328884591, w0=73.50000000000014, w1=12.304260318464541\n",
      "SubSGD iter. 227/499: loss=8.179473088544356, w0=74.20000000000014, w1=12.460765943900551\n",
      "SubSGD iter. 228/499: loss=6.398316073724487, w0=73.50000000000014, w1=12.900368913355681\n",
      "SubSGD iter. 229/499: loss=1.5799361794625426, w0=72.80000000000014, w1=13.928238303197134\n",
      "SubSGD iter. 230/499: loss=1.487547815685062, w0=72.10000000000014, w1=13.054202293354086\n",
      "SubSGD iter. 231/499: loss=10.442261107779899, w0=71.40000000000013, w1=13.706159532455507\n",
      "SubSGD iter. 232/499: loss=3.7688496312118787, w0=70.70000000000013, w1=13.565888467534844\n",
      "SubSGD iter. 233/499: loss=0.6168673981042616, w0=70.00000000000013, w1=12.926192861724525\n",
      "SubSGD iter. 234/499: loss=5.4249567900653375, w0=70.70000000000013, w1=12.106059634457406\n",
      "SubSGD iter. 235/499: loss=0.597773014674658, w0=71.40000000000013, w1=12.60488331617857\n",
      "SubSGD iter. 236/499: loss=3.6334614992700764, w0=72.10000000000014, w1=12.015976464164702\n",
      "SubSGD iter. 237/499: loss=6.690784558181676, w0=72.80000000000014, w1=10.92106579508546\n",
      "SubSGD iter. 238/499: loss=6.282359371799103, w0=72.10000000000014, w1=10.84988943925967\n",
      "SubSGD iter. 239/499: loss=1.2622765030806562, w0=71.40000000000013, w1=10.11844021505608\n",
      "SubSGD iter. 240/499: loss=12.334675612653186, w0=72.10000000000014, w1=11.056435604523\n",
      "SubSGD iter. 241/499: loss=1.8164126219796444, w0=72.80000000000014, w1=10.247901161468704\n",
      "SubSGD iter. 242/499: loss=1.8839945573501637, w0=72.10000000000014, w1=9.41078226185473\n",
      "SubSGD iter. 243/499: loss=11.06468444242735, w0=72.80000000000014, w1=9.585358536409263\n",
      "SubSGD iter. 244/499: loss=0.34193363855261083, w0=72.10000000000014, w1=10.467978921537739\n",
      "SubSGD iter. 245/499: loss=0.008025346740268446, w0=72.80000000000014, w1=10.657637401786268\n",
      "SubSGD iter. 246/499: loss=6.0763901685524715, w0=72.10000000000014, w1=11.941863244207967\n",
      "SubSGD iter. 247/499: loss=4.152884048676782, w0=71.40000000000013, w1=10.756652656549091\n",
      "SubSGD iter. 248/499: loss=1.0102766860395036, w0=70.70000000000013, w1=10.51169895674901\n",
      "SubSGD iter. 249/499: loss=4.778634675107142, w0=71.40000000000013, w1=11.160990272805083\n",
      "SubSGD iter. 250/499: loss=3.8881634994013865, w0=70.70000000000013, w1=11.131527412274476\n",
      "SubSGD iter. 251/499: loss=10.67431679204006, w0=71.40000000000013, w1=11.593172575138166\n",
      "SubSGD iter. 252/499: loss=2.018796730720098, w0=70.70000000000013, w1=11.886382616431572\n",
      "SubSGD iter. 253/499: loss=2.6466712869379165, w0=71.40000000000013, w1=11.476802593812593\n",
      "SubSGD iter. 254/499: loss=3.1673975229908606, w0=70.70000000000013, w1=12.282733722776358\n",
      "SubSGD iter. 255/499: loss=2.930711256299304, w0=71.40000000000013, w1=11.290358102118157\n",
      "SubSGD iter. 256/499: loss=9.515833633277765, w0=72.10000000000014, w1=12.337873959243515\n",
      "SubSGD iter. 257/499: loss=6.007770758012931, w0=71.40000000000013, w1=12.942940429491845\n",
      "SubSGD iter. 258/499: loss=4.592747134406778, w0=72.10000000000014, w1=13.227300874175192\n",
      "SubSGD iter. 259/499: loss=8.008968878066057, w0=71.40000000000013, w1=12.24457304715807\n",
      "SubSGD iter. 260/499: loss=4.351950157732304, w0=70.70000000000013, w1=12.773450598697845\n",
      "SubSGD iter. 261/499: loss=8.94280500492215, w0=71.40000000000013, w1=12.929818165768918\n",
      "SubSGD iter. 262/499: loss=3.8648884264118806, w0=72.10000000000014, w1=13.496938470597408\n",
      "SubSGD iter. 263/499: loss=0.3937377916790865, w0=72.80000000000014, w1=13.450963983848819\n",
      "SubSGD iter. 264/499: loss=6.512459140012666, w0=72.10000000000014, w1=13.154114994652078\n",
      "SubSGD iter. 265/499: loss=3.1373833879627, w0=71.40000000000013, w1=11.401557235093271\n",
      "SubSGD iter. 266/499: loss=0.7298887689823275, w0=70.70000000000013, w1=12.937965980435498\n",
      "SubSGD iter. 267/499: loss=4.508766162659654, w0=71.40000000000013, w1=11.590845767656766\n",
      "SubSGD iter. 268/499: loss=7.304640210254803, w0=72.10000000000014, w1=10.968801076040776\n",
      "SubSGD iter. 269/499: loss=2.2733355425547472, w0=72.80000000000014, w1=11.559287956134229\n",
      "SubSGD iter. 270/499: loss=7.071323841300909, w0=72.10000000000014, w1=11.480158414203633\n",
      "SubSGD iter. 271/499: loss=8.361210965981684, w0=71.40000000000013, w1=12.230443160254667\n",
      "SubSGD iter. 272/499: loss=0.6244563129072276, w0=70.70000000000013, w1=12.266263927862916\n",
      "SubSGD iter. 273/499: loss=0.7673150853403641, w0=70.00000000000013, w1=13.159801438661074\n",
      "SubSGD iter. 274/499: loss=7.307620522153826, w0=70.70000000000013, w1=13.625939039423171\n",
      "SubSGD iter. 275/499: loss=10.574797818031534, w0=71.40000000000013, w1=13.206023587960136\n",
      "SubSGD iter. 276/499: loss=1.0865329618461317, w0=72.10000000000014, w1=13.900711819705807\n",
      "SubSGD iter. 277/499: loss=1.550990856749607, w0=72.80000000000014, w1=13.855308217889279\n",
      "SubSGD iter. 278/499: loss=7.531387823342577, w0=72.10000000000014, w1=13.985111845437675\n",
      "SubSGD iter. 279/499: loss=2.0888052683850447, w0=71.40000000000013, w1=13.631270230876146\n",
      "SubSGD iter. 280/499: loss=1.9066249990471817, w0=72.10000000000014, w1=14.790023604301066\n",
      "SubSGD iter. 281/499: loss=6.437253196549904, w0=72.80000000000014, w1=13.89168426513087\n",
      "SubSGD iter. 282/499: loss=2.8172870694257455, w0=73.50000000000014, w1=13.788047691167597\n",
      "SubSGD iter. 283/499: loss=0.8279159684116308, w0=72.80000000000014, w1=12.354953377683987\n",
      "SubSGD iter. 284/499: loss=10.69368230767705, w0=73.50000000000014, w1=12.938690336032021\n",
      "SubSGD iter. 285/499: loss=1.2688634563405756, w0=72.80000000000014, w1=13.66267024859706\n",
      "SubSGD iter. 286/499: loss=1.5367595677955208, w0=72.10000000000014, w1=14.670966823997947\n",
      "SubSGD iter. 287/499: loss=6.093175165074726, w0=72.80000000000014, w1=15.212215313812335\n",
      "SubSGD iter. 288/499: loss=4.5105769649022704, w0=73.50000000000014, w1=15.469888623168575\n",
      "SubSGD iter. 289/499: loss=3.256582405112148, w0=72.80000000000014, w1=15.814119230956731\n",
      "SubSGD iter. 290/499: loss=3.820776651816054, w0=72.10000000000014, w1=16.107282076796068\n",
      "SubSGD iter. 291/499: loss=5.501064337506776, w0=72.80000000000014, w1=16.460494940221484\n",
      "SubSGD iter. 292/499: loss=10.753429298396355, w0=73.50000000000014, w1=15.527891784175036\n",
      "SubSGD iter. 293/499: loss=11.038833054978575, w0=72.80000000000014, w1=15.655600195742752\n",
      "SubSGD iter. 294/499: loss=0.3725337818622023, w0=73.50000000000014, w1=15.247154170145858\n",
      "SubSGD iter. 295/499: loss=3.207804471897475, w0=72.80000000000014, w1=15.755561993887891\n",
      "SubSGD iter. 296/499: loss=0.9220525600437242, w0=73.50000000000014, w1=16.141172731690112\n",
      "SubSGD iter. 297/499: loss=3.7562230110753347, w0=74.20000000000014, w1=16.4570398125552\n",
      "SubSGD iter. 298/499: loss=8.29690144366986, w0=73.50000000000014, w1=16.54930745465423\n",
      "SubSGD iter. 299/499: loss=0.7191875092437812, w0=74.20000000000014, w1=16.648962940166683\n",
      "SubSGD iter. 300/499: loss=7.934222611704442, w0=73.50000000000014, w1=16.99459675240791\n",
      "SubSGD iter. 301/499: loss=10.383220918330665, w0=74.20000000000014, w1=16.277837964655063\n",
      "SubSGD iter. 302/499: loss=9.825436360728872, w0=73.50000000000014, w1=15.16215315710557\n",
      "SubSGD iter. 303/499: loss=9.665607559752672, w0=72.80000000000014, w1=14.277546595003683\n",
      "SubSGD iter. 304/499: loss=3.8624186178163455, w0=73.50000000000014, w1=15.276250273363143\n",
      "SubSGD iter. 305/499: loss=3.61421539581643, w0=72.80000000000014, w1=15.504288879601287\n",
      "SubSGD iter. 306/499: loss=11.482406246045969, w0=73.50000000000014, w1=14.054506249071084\n",
      "SubSGD iter. 307/499: loss=10.412978091192144, w0=72.80000000000014, w1=13.864506788769278\n",
      "SubSGD iter. 308/499: loss=5.477116674230153, w0=72.10000000000014, w1=14.110217279165935\n",
      "SubSGD iter. 309/499: loss=9.523202110801549, w0=72.80000000000014, w1=14.280485375815923\n",
      "SubSGD iter. 310/499: loss=1.0911645407760204, w0=72.10000000000014, w1=14.03633847609225\n",
      "SubSGD iter. 311/499: loss=4.922269571664984, w0=72.80000000000014, w1=13.591828619490835\n",
      "SubSGD iter. 312/499: loss=0.38677322832056404, w0=73.50000000000014, w1=13.586702409619063\n",
      "SubSGD iter. 313/499: loss=0.7244469725675771, w0=74.20000000000014, w1=12.547375540853468\n",
      "SubSGD iter. 314/499: loss=6.132593199526994, w0=74.90000000000015, w1=13.126470214831802\n",
      "SubSGD iter. 315/499: loss=2.7142127097414033, w0=74.20000000000014, w1=13.71357465131632\n",
      "SubSGD iter. 316/499: loss=3.678158435291749, w0=73.50000000000014, w1=12.967762254627765\n",
      "SubSGD iter. 317/499: loss=1.2242938219758628, w0=72.80000000000014, w1=13.177319542534606\n",
      "SubSGD iter. 318/499: loss=1.9243540962585968, w0=73.50000000000014, w1=13.739727428421537\n",
      "SubSGD iter. 319/499: loss=3.891306357533381, w0=74.20000000000014, w1=14.289411946198841\n",
      "SubSGD iter. 320/499: loss=4.649487816558079, w0=74.90000000000015, w1=14.011759530028554\n",
      "SubSGD iter. 321/499: loss=1.68544183476385, w0=74.20000000000014, w1=13.764826924948478\n",
      "SubSGD iter. 322/499: loss=0.11349564706931403, w0=73.50000000000014, w1=14.056321359218401\n",
      "SubSGD iter. 323/499: loss=8.281482092409746, w0=72.80000000000014, w1=13.890061125339264\n",
      "SubSGD iter. 324/499: loss=5.162568240468104, w0=72.10000000000014, w1=13.352984409545375\n",
      "SubSGD iter. 325/499: loss=11.094052637473524, w0=71.40000000000013, w1=12.329786015224538\n",
      "SubSGD iter. 326/499: loss=6.829834246930346, w0=70.70000000000013, w1=12.558990582423808\n",
      "SubSGD iter. 327/499: loss=5.342845018764294, w0=71.40000000000013, w1=12.239096458887149\n",
      "SubSGD iter. 328/499: loss=5.196341587437757, w0=70.70000000000013, w1=12.316563078048592\n",
      "SubSGD iter. 329/499: loss=0.2657246876038357, w0=70.00000000000013, w1=13.447716278436975\n",
      "SubSGD iter. 330/499: loss=3.805363956201049, w0=70.70000000000013, w1=12.690538551073367\n",
      "SubSGD iter. 331/499: loss=3.5728922389215683, w0=70.00000000000013, w1=12.729839649493119\n",
      "SubSGD iter. 332/499: loss=6.7737269582123645, w0=69.30000000000013, w1=12.877451351316722\n",
      "SubSGD iter. 333/499: loss=9.80053631046318, w0=70.00000000000013, w1=12.078489355610527\n",
      "SubSGD iter. 334/499: loss=7.530587414920568, w0=69.30000000000013, w1=12.76832661768237\n",
      "SubSGD iter. 335/499: loss=5.317832019988003, w0=70.00000000000013, w1=12.068917480885998\n",
      "SubSGD iter. 336/499: loss=4.4416695502431764, w0=69.30000000000013, w1=12.323723853045216\n",
      "SubSGD iter. 337/499: loss=0.166001977173849, w0=68.60000000000012, w1=12.784878418959805\n",
      "SubSGD iter. 338/499: loss=2.4687820006114976, w0=67.90000000000012, w1=11.461693799432792\n",
      "SubSGD iter. 339/499: loss=3.4608579794991883, w0=68.60000000000012, w1=10.583151352916985\n",
      "SubSGD iter. 340/499: loss=7.679785247994062, w0=69.30000000000013, w1=10.896528520558036\n",
      "SubSGD iter. 341/499: loss=10.29475953520187, w0=70.00000000000013, w1=10.516801096011717\n",
      "SubSGD iter. 342/499: loss=3.214941296454029, w0=70.70000000000013, w1=11.887951852781466\n",
      "SubSGD iter. 343/499: loss=5.842726200835628, w0=71.40000000000013, w1=12.220298296873988\n",
      "SubSGD iter. 344/499: loss=6.029211691757148, w0=72.10000000000014, w1=11.020622041834821\n",
      "SubSGD iter. 345/499: loss=0.7057760822202539, w0=72.80000000000014, w1=9.844531630232067\n",
      "SubSGD iter. 346/499: loss=2.2213047795362826, w0=73.50000000000014, w1=9.992521108783809\n",
      "SubSGD iter. 347/499: loss=0.8623157256169023, w0=72.80000000000014, w1=11.046693418838986\n",
      "SubSGD iter. 348/499: loss=5.030546836719992, w0=73.50000000000014, w1=11.807525793591928\n",
      "SubSGD iter. 349/499: loss=1.8954267549206918, w0=72.80000000000014, w1=11.760061003789938\n",
      "SubSGD iter. 350/499: loss=8.138900282516403, w0=73.50000000000014, w1=10.435714512698873\n",
      "SubSGD iter. 351/499: loss=9.380531302017872, w0=72.80000000000014, w1=10.456967205734642\n",
      "SubSGD iter. 352/499: loss=11.783068049281098, w0=73.50000000000014, w1=11.378662500041665\n",
      "SubSGD iter. 353/499: loss=1.5900971666397439, w0=74.20000000000014, w1=10.585480404546265\n",
      "SubSGD iter. 354/499: loss=10.129395258872222, w0=74.90000000000015, w1=10.855589313931866\n",
      "SubSGD iter. 355/499: loss=1.4000038866781068, w0=74.20000000000014, w1=11.11892073645024\n",
      "SubSGD iter. 356/499: loss=1.579322780638293, w0=74.90000000000015, w1=12.84421511580522\n",
      "SubSGD iter. 357/499: loss=11.06250593413884, w0=75.60000000000015, w1=12.955038744208615\n",
      "SubSGD iter. 358/499: loss=10.44874933339463, w0=76.30000000000015, w1=12.469940286500512\n",
      "SubSGD iter. 359/499: loss=4.753736990949548, w0=75.60000000000015, w1=12.463921737749533\n",
      "SubSGD iter. 360/499: loss=4.6094375636349625, w0=74.90000000000015, w1=13.256405207440018\n",
      "SubSGD iter. 361/499: loss=6.876599642388811, w0=74.20000000000014, w1=11.999058036121479\n",
      "SubSGD iter. 362/499: loss=2.931502276030912, w0=73.50000000000014, w1=11.033329073998392\n",
      "SubSGD iter. 363/499: loss=4.841186347614297, w0=74.20000000000014, w1=11.882511168955872\n",
      "SubSGD iter. 364/499: loss=4.248126105163337, w0=74.90000000000015, w1=12.529991403827006\n",
      "SubSGD iter. 365/499: loss=4.842158370695202, w0=75.60000000000015, w1=13.718648786262289\n",
      "SubSGD iter. 366/499: loss=0.4443442009294998, w0=74.90000000000015, w1=14.36638192455438\n",
      "SubSGD iter. 367/499: loss=0.033091395336782625, w0=75.60000000000015, w1=14.819626572636558\n",
      "SubSGD iter. 368/499: loss=5.829924284206058, w0=74.90000000000015, w1=13.753584676477944\n",
      "SubSGD iter. 369/499: loss=20.42027892625414, w0=74.20000000000014, w1=13.689016531862274\n",
      "SubSGD iter. 370/499: loss=1.2397656798767684, w0=73.50000000000014, w1=12.779513373750142\n",
      "SubSGD iter. 371/499: loss=0.5741535480858602, w0=72.80000000000014, w1=12.483935368477376\n",
      "SubSGD iter. 372/499: loss=4.119529918306739, w0=73.50000000000014, w1=13.29764621923931\n",
      "SubSGD iter. 373/499: loss=7.011671399852673, w0=72.80000000000014, w1=13.116146268714926\n",
      "SubSGD iter. 374/499: loss=0.6813201698638238, w0=73.50000000000014, w1=13.463259198505451\n",
      "SubSGD iter. 375/499: loss=9.932995460774137, w0=74.20000000000014, w1=13.68006184189811\n",
      "SubSGD iter. 376/499: loss=7.636626542491108, w0=73.50000000000014, w1=14.357351994959117\n",
      "SubSGD iter. 377/499: loss=0.5103685225944332, w0=74.20000000000014, w1=14.428491670451669\n",
      "SubSGD iter. 378/499: loss=1.358860993347676, w0=74.90000000000015, w1=15.418149378171812\n",
      "SubSGD iter. 379/499: loss=2.900806404382436, w0=74.20000000000014, w1=14.713060279030854\n",
      "SubSGD iter. 380/499: loss=8.608895980776055, w0=73.50000000000014, w1=13.06535152933777\n",
      "SubSGD iter. 381/499: loss=11.915598999502606, w0=74.20000000000014, w1=13.095058503298576\n",
      "SubSGD iter. 382/499: loss=3.5290790489927275, w0=73.50000000000014, w1=12.142032931944735\n",
      "SubSGD iter. 383/499: loss=3.1931642713615744, w0=72.80000000000014, w1=12.926822524807847\n",
      "SubSGD iter. 384/499: loss=16.44477204893363, w0=72.10000000000014, w1=12.923206288464932\n",
      "SubSGD iter. 385/499: loss=9.617582014642608, w0=72.80000000000014, w1=13.862809648516887\n",
      "SubSGD iter. 386/499: loss=3.4633188999293765, w0=73.50000000000014, w1=13.624428932642092\n",
      "SubSGD iter. 387/499: loss=1.4003721263763111, w0=74.20000000000014, w1=13.226694656978976\n",
      "SubSGD iter. 388/499: loss=9.634101270117938, w0=73.50000000000014, w1=13.134374198939561\n",
      "SubSGD iter. 389/499: loss=0.16447204219957712, w0=72.80000000000014, w1=13.073830369261408\n",
      "SubSGD iter. 390/499: loss=0.1871662366872897, w0=72.10000000000014, w1=13.313076075954486\n",
      "SubSGD iter. 391/499: loss=6.724773414945815, w0=71.40000000000013, w1=14.171592529792983\n",
      "SubSGD iter. 392/499: loss=2.2673814711208777, w0=70.70000000000013, w1=14.849240499917471\n",
      "SubSGD iter. 393/499: loss=5.219180518682492, w0=70.00000000000013, w1=13.788566422908051\n",
      "SubSGD iter. 394/499: loss=10.741559162765824, w0=70.70000000000013, w1=14.462534523434188\n",
      "SubSGD iter. 395/499: loss=4.841435843122824, w0=70.00000000000013, w1=14.165685534237447\n",
      "SubSGD iter. 396/499: loss=6.969636910103688, w0=70.70000000000013, w1=12.580465859707234\n",
      "SubSGD iter. 397/499: loss=4.4075614126031155, w0=71.40000000000013, w1=13.933075854007383\n",
      "SubSGD iter. 398/499: loss=1.2645840194339257, w0=70.70000000000013, w1=12.693271030170761\n",
      "SubSGD iter. 399/499: loss=2.3097327199643516, w0=71.40000000000013, w1=11.965350341336604\n",
      "SubSGD iter. 400/499: loss=3.2914853736291434, w0=72.10000000000014, w1=11.70457167414099\n",
      "SubSGD iter. 401/499: loss=1.5769252932847877, w0=71.40000000000013, w1=10.986346175641444\n",
      "SubSGD iter. 402/499: loss=5.573289493251238, w0=72.10000000000014, w1=11.23586497595364\n",
      "SubSGD iter. 403/499: loss=11.012301209156334, w0=72.80000000000014, w1=11.942894965431364\n",
      "SubSGD iter. 404/499: loss=1.9480437469805878, w0=73.50000000000014, w1=12.953169015904301\n",
      "SubSGD iter. 405/499: loss=3.524252972491759, w0=72.80000000000014, w1=14.154848154496987\n",
      "SubSGD iter. 406/499: loss=2.3651650758660807, w0=73.50000000000014, w1=13.393670969386939\n",
      "SubSGD iter. 407/499: loss=7.330743172865368, w0=74.20000000000014, w1=14.170916256887683\n",
      "SubSGD iter. 408/499: loss=1.8234458135971607, w0=74.90000000000015, w1=15.230195164642273\n",
      "SubSGD iter. 409/499: loss=5.391758270992938, w0=75.60000000000015, w1=15.553413935902588\n",
      "SubSGD iter. 410/499: loss=1.5781596298939036, w0=76.30000000000015, w1=15.269390305515804\n",
      "SubSGD iter. 411/499: loss=8.588734271369326, w0=77.00000000000016, w1=14.062759935443895\n",
      "SubSGD iter. 412/499: loss=2.8284537022681704, w0=76.30000000000015, w1=14.704155716257462\n",
      "SubSGD iter. 413/499: loss=3.83906091586735, w0=77.00000000000016, w1=14.3618041313313\n",
      "SubSGD iter. 414/499: loss=5.735602704421275, w0=76.30000000000015, w1=15.550175421148417\n",
      "SubSGD iter. 415/499: loss=6.336487178467898, w0=75.60000000000015, w1=16.703178041292055\n",
      "SubSGD iter. 416/499: loss=6.979212153393604, w0=74.90000000000015, w1=16.90565086514957\n",
      "SubSGD iter. 417/499: loss=2.927333800892427, w0=75.60000000000015, w1=17.767442403825612\n",
      "SubSGD iter. 418/499: loss=10.170097960572527, w0=74.90000000000015, w1=17.89688356197542\n",
      "SubSGD iter. 419/499: loss=1.5553366921875664, w0=74.20000000000014, w1=17.996504557331672\n",
      "SubSGD iter. 420/499: loss=14.77004989229441, w0=74.90000000000015, w1=17.861401222772532\n",
      "SubSGD iter. 421/499: loss=9.018098638264334, w0=74.20000000000014, w1=16.878502512509577\n",
      "SubSGD iter. 422/499: loss=3.3641186877798575, w0=74.90000000000015, w1=16.411997613644715\n",
      "SubSGD iter. 423/499: loss=1.0115365419148077, w0=75.60000000000015, w1=14.978807143534208\n",
      "SubSGD iter. 424/499: loss=4.056556272227482, w0=74.90000000000015, w1=13.682702003304692\n",
      "SubSGD iter. 425/499: loss=11.166973661084214, w0=74.20000000000014, w1=12.551043410851085\n",
      "SubSGD iter. 426/499: loss=0.2383351732526222, w0=74.90000000000015, w1=13.560968142718153\n",
      "SubSGD iter. 427/499: loss=2.4663221085532996, w0=74.20000000000014, w1=12.737429146082416\n",
      "SubSGD iter. 428/499: loss=9.685723434540051, w0=74.90000000000015, w1=12.401430758579231\n",
      "SubSGD iter. 429/499: loss=1.7024498121978127, w0=75.60000000000015, w1=13.163355113963712\n",
      "SubSGD iter. 430/499: loss=4.072737952640281, w0=74.90000000000015, w1=11.769243263224029\n",
      "SubSGD iter. 431/499: loss=5.4137395324524675, w0=75.60000000000015, w1=12.175804730694345\n",
      "SubSGD iter. 432/499: loss=7.637984266008196, w0=74.90000000000015, w1=12.882898867901558\n",
      "SubSGD iter. 433/499: loss=1.6434697267057032, w0=74.20000000000014, w1=12.202932593230674\n",
      "SubSGD iter. 434/499: loss=2.2020961796027336, w0=73.50000000000014, w1=10.346791065723195\n",
      "SubSGD iter. 435/499: loss=9.10787326977173, w0=74.20000000000014, w1=11.419101283429292\n",
      "SubSGD iter. 436/499: loss=9.284472096127246, w0=73.50000000000014, w1=11.60040314873536\n",
      "SubSGD iter. 437/499: loss=5.47961972711466, w0=72.80000000000014, w1=12.452022310219194\n",
      "SubSGD iter. 438/499: loss=3.092539812527626, w0=73.50000000000014, w1=11.453273160104713\n",
      "SubSGD iter. 439/499: loss=5.67378292681974, w0=74.20000000000014, w1=12.034706889078224\n",
      "SubSGD iter. 440/499: loss=9.823936805751138, w0=73.50000000000014, w1=12.542596296963564\n",
      "SubSGD iter. 441/499: loss=6.2360283328359, w0=74.20000000000014, w1=11.041764833637687\n",
      "SubSGD iter. 442/499: loss=0.23122928599485704, w0=74.90000000000015, w1=10.313676157244704\n",
      "SubSGD iter. 443/499: loss=1.7849699999016835, w0=74.20000000000014, w1=11.038182182266457\n",
      "SubSGD iter. 444/499: loss=6.440752123278791, w0=73.50000000000014, w1=11.636199540321519\n",
      "SubSGD iter. 445/499: loss=8.168620551729838, w0=72.80000000000014, w1=11.827945858909025\n",
      "SubSGD iter. 446/499: loss=0.6393620026538969, w0=73.50000000000014, w1=11.04590902095971\n",
      "SubSGD iter. 447/499: loss=12.503721701759467, w0=72.80000000000014, w1=11.798583727243328\n",
      "SubSGD iter. 448/499: loss=6.971962170260994, w0=72.10000000000014, w1=12.477532284620741\n",
      "SubSGD iter. 449/499: loss=8.479623545582008, w0=71.40000000000013, w1=13.405008307754994\n",
      "SubSGD iter. 450/499: loss=2.162197000581145, w0=70.70000000000013, w1=12.810347448375946\n",
      "SubSGD iter. 451/499: loss=1.667252179195252, w0=71.40000000000013, w1=11.557455029329791\n",
      "SubSGD iter. 452/499: loss=5.430549158120954, w0=70.70000000000013, w1=11.10820127434509\n",
      "SubSGD iter. 453/499: loss=0.4598267285982871, w0=71.40000000000013, w1=11.477111884199573\n",
      "SubSGD iter. 454/499: loss=4.74479026598884, w0=72.10000000000014, w1=13.144748021162584\n",
      "SubSGD iter. 455/499: loss=2.1886112439399312, w0=72.80000000000014, w1=13.074611995537209\n",
      "SubSGD iter. 456/499: loss=1.2616829061041557, w0=72.10000000000014, w1=13.488875519502065\n",
      "SubSGD iter. 457/499: loss=13.808852603905223, w0=72.80000000000014, w1=12.791953139618773\n",
      "SubSGD iter. 458/499: loss=3.011025099311823, w0=73.50000000000014, w1=13.144459075354842\n",
      "SubSGD iter. 459/499: loss=8.297047601945266, w0=72.80000000000014, w1=13.200495955742674\n",
      "SubSGD iter. 460/499: loss=11.581629880915585, w0=73.50000000000014, w1=12.91250991015189\n",
      "SubSGD iter. 461/499: loss=4.185757297463979, w0=74.20000000000014, w1=13.031026454199502\n",
      "SubSGD iter. 462/499: loss=7.453250718485151, w0=73.50000000000014, w1=12.34423210867863\n",
      "SubSGD iter. 463/499: loss=5.702995593082669, w0=72.80000000000014, w1=13.103918440548322\n",
      "SubSGD iter. 464/499: loss=0.9636392038217352, w0=73.50000000000014, w1=12.367706630200347\n",
      "SubSGD iter. 465/499: loss=7.085367468340323, w0=72.80000000000014, w1=12.125163928135622\n",
      "SubSGD iter. 466/499: loss=6.121967142789472, w0=72.10000000000014, w1=13.386060171393279\n",
      "SubSGD iter. 467/499: loss=2.6946372899226674, w0=71.40000000000013, w1=13.898312809962752\n",
      "SubSGD iter. 468/499: loss=0.6853953791826797, w0=72.10000000000014, w1=14.823383934498365\n",
      "SubSGD iter. 469/499: loss=5.754440265381419, w0=72.80000000000014, w1=15.708117767504568\n",
      "SubSGD iter. 470/499: loss=1.5044366771206867, w0=72.10000000000014, w1=16.546861000578687\n",
      "SubSGD iter. 471/499: loss=4.548036233284364, w0=71.40000000000013, w1=16.15616745334609\n",
      "SubSGD iter. 472/499: loss=8.027161555971063, w0=72.10000000000014, w1=15.717439482225162\n",
      "SubSGD iter. 473/499: loss=7.404371485929943, w0=72.80000000000014, w1=14.950080508329114\n",
      "SubSGD iter. 474/499: loss=6.73134955804133, w0=72.10000000000014, w1=14.292391861842185\n",
      "SubSGD iter. 475/499: loss=0.5017017294778228, w0=72.80000000000014, w1=13.466350737516061\n",
      "SubSGD iter. 476/499: loss=1.666305267183489, w0=72.10000000000014, w1=14.491790740671695\n",
      "SubSGD iter. 477/499: loss=5.613212873354513, w0=72.80000000000014, w1=14.655390516227909\n",
      "SubSGD iter. 478/499: loss=3.612740804714143, w0=72.10000000000014, w1=15.01958235262213\n",
      "SubSGD iter. 479/499: loss=11.082521581076676, w0=72.80000000000014, w1=13.01254484281577\n",
      "SubSGD iter. 480/499: loss=0.8164343303567563, w0=73.50000000000014, w1=14.357473822284105\n",
      "SubSGD iter. 481/499: loss=6.09636403006256, w0=72.80000000000014, w1=14.148245381349437\n",
      "SubSGD iter. 482/499: loss=3.398367899483489, w0=72.10000000000014, w1=14.403966108954176\n",
      "SubSGD iter. 483/499: loss=12.266137466844214, w0=72.80000000000014, w1=14.566996217155745\n",
      "SubSGD iter. 484/499: loss=3.529215048532791, w0=72.10000000000014, w1=14.56351281953768\n",
      "SubSGD iter. 485/499: loss=1.6559286959288357, w0=71.40000000000013, w1=15.181947863703384\n",
      "SubSGD iter. 486/499: loss=3.861068162798375, w0=70.70000000000013, w1=14.982875982840829\n",
      "SubSGD iter. 487/499: loss=7.144005465793484, w0=70.00000000000013, w1=14.743428972208466\n",
      "SubSGD iter. 488/499: loss=3.971132249913495, w0=70.70000000000013, w1=14.814568647701018\n",
      "SubSGD iter. 489/499: loss=11.632660363661358, w0=71.40000000000013, w1=14.34143976529379\n",
      "SubSGD iter. 490/499: loss=1.8961657002654562, w0=72.10000000000014, w1=15.116374754145726\n",
      "SubSGD iter. 491/499: loss=2.426330862481798, w0=72.80000000000014, w1=14.661379009623209\n",
      "SubSGD iter. 492/499: loss=6.763038860098064, w0=72.10000000000014, w1=14.779101472646122\n",
      "SubSGD iter. 493/499: loss=2.856851234501505, w0=71.40000000000013, w1=13.64251455841915\n",
      "SubSGD iter. 494/499: loss=4.793287521112717, w0=72.10000000000014, w1=13.021534607040302\n",
      "SubSGD iter. 495/499: loss=0.4949601396132124, w0=72.80000000000014, w1=12.782288900347224\n",
      "SubSGD iter. 496/499: loss=4.617482819832674, w0=73.50000000000014, w1=12.302832464752143\n",
      "SubSGD iter. 497/499: loss=3.5820190571075585, w0=72.80000000000014, w1=12.228522196077352\n",
      "SubSGD iter. 498/499: loss=4.636009979006246, w0=73.50000000000014, w1=12.629053850977927\n",
      "SubSGD iter. 499/499: loss=5.193733352406554, w0=72.80000000000014, w1=11.7544023957004\n",
      "SubSGD: execution time=0.349 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e659607ec6484ee1bd1e8d58fafba336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9479987f2ed20999a713725a6756863f6b1eb46c4b0caa63864f6802c4dcff56"
   }
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
